"""
ğŸ”— è´£ä»»é“¾æ¨¡å¼ - å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†æµæ°´çº¿

ğŸ¯ è´£ä»»é“¾æ¨¡å¼æ ¸å¿ƒæ€æƒ³ï¼š
å°†è¯·æ±‚æ²¿ç€å¤„ç†è€…é“¾ä¼ é€’ï¼Œç›´åˆ°æœ‰ä¸€ä¸ªå¤„ç†è€…èƒ½å¤Ÿå¤„ç†å®ƒã€‚æ¯ä¸ªå¤„ç†è€…éƒ½æœ‰æœºä¼šå¤„ç†è¯·æ±‚ï¼Œ
å¦‚æœæ— æ³•å¤„ç†å°±ä¼ é€’ç»™é“¾ä¸­çš„ä¸‹ä¸€ä¸ªå¤„ç†è€…ã€‚

ğŸ—ï¸ AIå·¥ä½œæµä¸­çš„åº”ç”¨åœºæ™¯ï¼š
1. æ–‡æ¡£é¢„å¤„ç†æµæ°´çº¿ï¼šæ ¼å¼éªŒè¯ â†’ å†…å®¹æå– â†’ AIåˆ†æ â†’ ç»“æœæ•´åˆ
2. å¤šçº§AIå¤„ç†ï¼šåŸºç¡€åˆ†æ â†’ æ·±åº¦åˆ†æ â†’ ç»“æœä¼˜åŒ– â†’ è¾“å‡ºæ ¼å¼åŒ–
3. é”™è¯¯å¤„ç†é“¾ï¼šé‡è¯•æœºåˆ¶ â†’ é™çº§å¤„ç† â†’ é”™è¯¯è®°å½• â†’ ç”¨æˆ·é€šçŸ¥

ğŸ’¡ å¢å¼ºç‰¹æ€§ï¼š
- è¯¦ç»†çš„å¤„ç†é“¾æ‰§è¡Œæµç¨‹æ‰“å°
- JSONæ ¼å¼çš„ä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœ
- AIæœåŠ¡é›†æˆçŠ¶æ€è·Ÿè¸ª
- å¤„ç†æ€§èƒ½ç»Ÿè®¡å’Œåˆ†æ
- é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import re
import json
import time
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ai_service import get_ai_service


def create_safe_json_output(output_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ›å»ºå®‰å…¨çš„JSONè¾“å‡ºï¼Œé¿å…å¾ªç¯å¼•ç”¨

    Args:
        output_data: åŸå§‹è¾“å‡ºæ•°æ®

    Returns:
        Dict[str, Any]: å®‰å…¨çš„å¯åºåˆ—åŒ–æ•°æ®
    """
    # ç®€åŒ–ç‰ˆæœ¬ï¼šåªä¿ç•™é¡¶å±‚å¯å®‰å…¨åºåˆ—åŒ–çš„æ•°æ®
    safe_output = {}

    # åªå¤åˆ¶å®‰å…¨çš„é”®å€¼å¯¹
    safe_keys = ['status', 'summary', 'processed_at']
    for key in safe_keys:
        if key in output_data:
            safe_output[key] = output_data[key]

    # å¯¹äºdetailed_resultsï¼Œåªä¿ç•™åŸºæœ¬ä¿¡æ¯
    if 'detailed_results' in output_data:
        safe_output['detailed_results'] = {
            'format_validation': {
                'valid': output_data['detailed_results'].get('format_validation', {}).get('summary', {}).get('valid', False),
                'format': output_data['detailed_results'].get('format_validation', {}).get('summary', {}).get('format', 'unknown')
            },
            'sentiment_analysis': {
                'sentiment': output_data['detailed_results'].get('sentiment_analysis', {}).get('sentiment', 'neutral'),
                'confidence': output_data['detailed_results'].get('sentiment_analysis', {}).get('confidence', 0)
            },
            'ai_summary': {
                'compression_ratio': output_data['detailed_results'].get('ai_summary', {}).get('summary_result', {}).get('compression_ratio', 0),
                'service_statistics': output_data['detailed_results'].get('ai_summary', {}).get('service_statistics', {})
            }
        }

    return safe_output


@dataclass
class ProcessingRequest:
    """ğŸ·ï¸ è´£ä»»é“¾æ¨¡å¼ - å¢å¼ºç‰ˆå¤„ç†è¯·æ±‚å¯¹è±¡

    è´£ä»»é“¾æ¨¡å¼è®¾è®¡è¦ç‚¹ï¼š
    1. æ•°æ®å°è£…ï¼šå°†æ‰€æœ‰å¤„ç†æ‰€éœ€æ•°æ®å°è£…åœ¨è¯·æ±‚å¯¹è±¡ä¸­
    2. å…ƒæ•°æ®ä¼ é€’ï¼šé€šè¿‡metadataå’Œresultså­—æ®µåœ¨å¤„ç†å™¨é—´ä¼ é€’ä¿¡æ¯
    3. å¤„ç†ç»Ÿè®¡ï¼šè®°å½•æ¯ä¸ªå¤„ç†å™¨çš„æ‰§è¡Œæ—¶é—´å’ŒçŠ¶æ€
    4. JSONåºåˆ—åŒ–ï¼šæ”¯æŒè¯¦ç»†çš„è°ƒè¯•è¾“å‡ºå’Œç»“æœåˆ†æ
    """
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    results: Dict[str, Any] = field(default_factory=dict)
    processing_stats: Dict[str, Any] = field(default_factory=dict)
    start_time: float = field(default_factory=time.time)

    def add_result(self, key: str, value: Any) -> None:
        """æ·»åŠ å¤„ç†ç»“æœ - è´£ä»»é“¾ä¸­å¤„ç†å™¨é—´ä¼ é€’ä¿¡æ¯çš„å…³é”®æœºåˆ¶"""
        self.results[key] = value

    def add_processing_stat(self, handler_name: str, stat_info: Dict[str, Any]) -> None:
        """æ·»åŠ å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        self.processing_stats[handler_name] = stat_info

    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONæ ¼å¼ - ç”¨äºè¯¦ç»†è¾“å‡ºå’Œè°ƒè¯•"""
        request_dict = {
            "content_info": {
                "length": len(self.content),
                "preview": self.content[:100] + "..." if len(self.content) > 100 else self.content,
                "word_count": len(self.content.split())
            },
            "metadata": self.metadata,
            "processing_results": self.results,
            "processing_stats": self.processing_stats,
            "elapsed_time": time.time() - self.start_time
        }
        return json.dumps(request_dict, ensure_ascii=False, indent=2)


class ProcessingResult(Enum):
    """å¤„ç†ç»“æœæšä¸¾"""
    CONTINUE = "continue"  # ç»§ç»­ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå¤„ç†è€…
    STOP = "stop"          # åœæ­¢å¤„ç†
    ERROR = "error"        # å¤„ç†å‡ºé”™


class DocumentHandler(ABC):
    """ğŸ”§ è´£ä»»é“¾æ¨¡å¼ - å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨æŠ½è±¡åŸºç±»

    è´£ä»»é“¾æ¨¡å¼è®¾è®¡è¦ç‚¹ï¼š
    1. æŠ½è±¡æ¥å£ï¼šå®šä¹‰ç»Ÿä¸€çš„å¤„ç†æ¥å£handle()å’Œæµç¨‹æ§åˆ¶process()
    2. é“¾å¼è¿æ¥ï¼šé€šè¿‡next_handlerå½¢æˆå¤„ç†é“¾
    3. è´£ä»»ä¼ é€’ï¼šæ— æ³•å¤„ç†æ—¶è‡ªåŠ¨ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå¤„ç†å™¨
    4. ç»Ÿè®¡å¢å¼ºï¼šè®°å½•æ¯ä¸ªå¤„ç†å™¨çš„æ‰§è¡Œæ—¶é—´å’Œè¯¦ç»†ä¿¡æ¯
    """

    def __init__(self, name: str):
        self.name = name
        self.next_handler: Optional['DocumentHandler'] = None
        self.handler_id = f"{name}_{int(time.time() * 1000)}"  # å”¯ä¸€æ ‡è¯†ç¬¦

    def set_next(self, handler: 'DocumentHandler') -> 'DocumentHandler':
        """è®¾ç½®ä¸‹ä¸€ä¸ªå¤„ç†å™¨ - è´£ä»»é“¾è¿æ¥çš„å…³é”®æ–¹æ³•"""
        print(f"ğŸ”— è¿æ¥å¤„ç†å™¨: {self.name} â¡ï¸ {handler.name}")
        self.next_handler = handler
        return handler

    @abstractmethod
    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        """å¤„ç†è¯·æ±‚çš„æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°"""
        pass

    def process(self, request: ProcessingRequest) -> ProcessingResult:
        """ğŸ“Š å¢å¼ºç‰ˆå¤„ç†æµç¨‹ - åŒ…å«è¯¦ç»†ç»Ÿè®¡å’Œé”™è¯¯å¤„ç†"""
        handler_start_time = time.time()

        print(f"\nğŸ”— ã€{self.name}ã€‘å¼€å§‹å¤„ç†")
        print(f"   ğŸ“‹ å¤„ç†å™¨ID: {self.handler_id}")
        print(f"   â±ï¸  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")

        try:
            # è®°å½•å¤„ç†å™¨å¼€å§‹æ‰§è¡Œ
            request.add_processing_stat(self.name, {
                "start_time": handler_start_time,
                "status": "processing",
                "handler_id": self.handler_id
            })

            # æ‰§è¡Œå…·ä½“çš„å¤„ç†é€»è¾‘
            result = self.handle(request)

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - handler_start_time

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            request.processing_stats[self.name].update({
                "end_time": time.time(),
                "processing_time": processing_time,
                "status": "completed" if result != ProcessingResult.ERROR else "failed",
                "result": result.value
            })

            print(f"   âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}ç§’")

            # å†³å®šæ˜¯å¦ç»§ç»­ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå¤„ç†å™¨
            if result == ProcessingResult.CONTINUE and self.next_handler:
                print(f"   â¡ï¸  ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå¤„ç†å™¨: {self.next_handler.name}")
                return self.next_handler.process(request)
            elif result == ProcessingResult.STOP:
                print(f"   â¹ï¸  å¤„ç†é“¾åœ¨ {self.name} å¤„åœæ­¢")
                return result
            else:
                print(f"   âŒ {self.name} å¤„ç†å¤±è´¥")
                return result

        except Exception as e:
            # è®°å½•é”™è¯¯ä¿¡æ¯
            error_time = time.time()
            processing_time = error_time - handler_start_time

            request.processing_stats[self.name].update({
                "end_time": error_time,
                "processing_time": processing_time,
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__
            })

            print(f"   ğŸ’¥ {self.name} å¤„ç†å‡ºé”™: {str(e)}")
            print(f"   ğŸ“ é”™è¯¯ç±»å‹: {type(e).__name__}")
            request.results["error"] = f"{self.name}: {str(e)}"
            return ProcessingResult.ERROR

    def print_detailed_stats(self, request: ProcessingRequest) -> None:
        """æ‰“å°è¯¦ç»†çš„å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = request.processing_stats.get(self.name, {})
        if stats:
            print(f"\nğŸ“ˆ ã€{self.name}ã€‘è¯¦ç»†ç»Ÿè®¡:")
            print(json.dumps(stats, ensure_ascii=False, indent=6))


class FormatValidationHandler(DocumentHandler):
    """ğŸ“ æ ¼å¼éªŒè¯å¤„ç†å™¨ - è´£ä»»é“¾çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹

    è´£ä»»é“¾æ¨¡å¼åº”ç”¨ï¼š
    1. å…¥å£éªŒè¯ï¼šä½œä¸ºå¤„ç†é“¾çš„å…¥å£ï¼Œç¡®ä¿è¾“å…¥æ•°æ®ç¬¦åˆè¦æ±‚
    2. å¿«é€Ÿå¤±è´¥ï¼šæå‰å‘ç°é—®é¢˜ï¼Œé¿å…æ— æ„ä¹‰çš„åç»­å¤„ç†
    3. æ ‡å‡†åŒ–è¾“å‡ºï¼šä¸ºåç»­å¤„ç†å™¨æä¾›ç»Ÿä¸€çš„éªŒè¯ç»“æœæ ¼å¼
    4. å¤šæ ¼å¼æ”¯æŒï¼šæ”¯æŒtextã€markdownã€htmlç­‰å¤šç§æ ¼å¼éªŒè¯
    """

    def __init__(self):
        super().__init__("æ ¼å¼éªŒè¯")
        self.supported_formats = ['text', 'markdown', 'html']
        self.max_size = 1000000  # 1MB

        print(f"ğŸ—ï¸  åˆå§‹åŒ–{self.name}å¤„ç†å™¨")
        print(f"   ğŸ“‹ æ”¯æŒæ ¼å¼: {', '.join(self.supported_formats)}")
        print(f"   ğŸ“ æœ€å¤§æ–‡æ¡£å¤§å°: {self.max_size:,} å­—ç¬¦")

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        """ğŸ” æ‰§è¡Œæ ¼å¼éªŒè¯ - è´£ä»»é“¾çš„ç¬¬ä¸€ä¸ªå¤„ç†æ­¥éª¤"""
        content = request.content
        metadata = request.metadata

        print(f"   ğŸ“Š å¼€å§‹éªŒè¯æ–‡æ¡£...")
        print(f"   ğŸ“ æ–‡æ¡£å¤§å°: {len(content):,} å­—ç¬¦")
        print(f"   ğŸ“ æ–‡æ¡£æ ¼å¼: {metadata.get('format', 'text')}")

        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'validation_steps': [],
            'warnings': [],
            'errors': []
        }

        # æ­¥éª¤1: æ–‡æ¡£å¤§å°æ£€æŸ¥
        size_check = {
            'step': 'size_validation',
            'max_size': self.max_size,
            'actual_size': len(content),
            'passed': len(content) <= self.max_size
        }
        validation_results['validation_steps'].append(size_check)

        if not size_check['passed']:
            error_msg = f"æ–‡æ¡£è¿‡å¤§: {len(content):,} > {self.max_size:,}"
            validation_results['errors'].append(error_msg)
            raise ValueError(error_msg)

        # æ­¥éª¤2: æ ¼å¼æ£€æŸ¥
        doc_format = metadata.get('format', 'text')
        format_check = {
            'step': 'format_validation',
            'requested_format': doc_format,
            'supported_formats': self.supported_formats,
            'passed': doc_format in self.supported_formats
        }
        validation_results['validation_steps'].append(format_check)

        if not format_check['passed']:
            error_msg = f"ä¸æ”¯æŒçš„æ ¼å¼: {doc_format}"
            validation_results['errors'].append(error_msg)
            raise ValueError(error_msg)

        # æ­¥éª¤3: å†…å®¹åŸºæœ¬æ£€æŸ¥
        content_is_empty = not content.strip()
        content_check = {
            'step': 'content_validation',
            'is_empty': content_is_empty,
            'content_length': len(content.strip()),
            'passed': not content_is_empty
        }
        validation_results['validation_steps'].append(content_check)

        if content_is_empty:
            error_msg = "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            validation_results['errors'].append(error_msg)
            raise ValueError(error_msg)

        # æ­¥éª¤4: æ ¼å¼ç‰¹å®šéªŒè¯
        if doc_format == 'html':
            html_validation = self._validate_html(content)
            html_check = {
                'step': 'html_specific_validation',
                'has_html_tags': html_validation,
                'passed': html_validation
            }
            validation_results['validation_steps'].append(html_check)

            if not html_validation:
                error_msg = "HTMLæ ¼å¼æ— æ•ˆ"
                validation_results['errors'].append(error_msg)
                raise ValueError(error_msg)

        elif doc_format == 'markdown':
            markdown_validation = self._validate_markdown(content)
            markdown_check = {
                'step': 'markdown_specific_validation',
                'has_markdown_syntax': markdown_validation,
                'passed': markdown_validation
            }
            validation_results['validation_steps'].append(markdown_check)

            if not markdown_validation:
                warning_msg = "æ–‡æ¡£ä¸åŒ…å«æ ‡å‡†Markdownè¯­æ³•ï¼Œå°†ä½œä¸ºçº¯æ–‡æœ¬å¤„ç†"
                validation_results['warnings'].append(warning_msg)
                print(f"   âš ï¸  {warning_msg}")

        # å®ŒæˆéªŒè¯ï¼Œè®¡ç®—æ€»ä½“ç»Ÿè®¡
        word_count = len(content.split())
        validation_results['summary'] = {
            'valid': len(validation_results['errors']) == 0,
            'format': doc_format,
            'size': len(content),
            'word_count': word_count,
            'line_count': content.count('\n') + 1,
            'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
            'total_warnings': len(validation_results['warnings']),
            'total_errors': len(validation_results['errors']),
            'validation_time': time.time()
        }

        # ä¿å­˜éªŒè¯ç»“æœåˆ°è¯·æ±‚å¯¹è±¡
        request.add_result('format_validation', validation_results)

        # æ‰“å°è¯¦ç»†çš„éªŒè¯ç»“æœ
        print(f"   âœ… æ ¼å¼éªŒè¯é€šè¿‡!")
        print(f"   ğŸ“Š éªŒè¯ç»Ÿè®¡:")
        print(f"      â€¢ æ ¼å¼: {validation_results['summary']['format']}")
        print(f"      â€¢ å¤§å°: {validation_results['summary']['size']:,} å­—ç¬¦")
        print(f"      â€¢ è¯æ±‡: {validation_results['summary']['word_count']:,} è¯")
        print(f"      â€¢ æ®µè½: {validation_results['summary']['paragraph_count']} ä¸ª")
        print(f"      â€¢ è­¦å‘Š: {validation_results['summary']['total_warnings']} ä¸ª")
        print(f"      â€¢ é”™è¯¯: {validation_results['summary']['total_errors']} ä¸ª")

        if validation_results['warnings']:
            print(f"   âš ï¸  è­¦å‘Šä¿¡æ¯:")
            for warning in validation_results['warnings']:
                print(f"      â€¢ {warning}")

        # æ‰“å°JSONæ ¼å¼çš„è¯¦ç»†éªŒè¯ç»“æœ
        print(f"   ğŸ“„ è¯¦ç»†éªŒè¯ç»“æœ (JSON):")
        validation_json = json.dumps(validation_results, ensure_ascii=False, indent=8)
        print(f"      {validation_json}")

        return ProcessingResult.CONTINUE

    def _validate_html(self, content: str) -> bool:
        """ç®€å•HTMLæ ¼å¼éªŒè¯"""
        # æ£€æŸ¥åŸºæœ¬çš„HTMLæ ‡ç­¾
        html_pattern = r'<[^>]+>'
        return bool(re.search(html_pattern, content))

    def _validate_markdown(self, content: str) -> bool:
        """ç®€å•Markdownæ ¼å¼éªŒè¯"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«Markdownè¯­æ³•
        md_patterns = [r'^#+\s', r'\*\*.*?\*\*', r'\*.*?\*', r'\[.*?\]\(.*?\)']
        return any(re.search(pattern, content, re.MULTILINE) for pattern in md_patterns)


class ContentExtractionHandler(DocumentHandler):
    """å†…å®¹æå–å¤„ç†å™¨"""

    def __init__(self):
        super().__init__("å†…å®¹æå–")

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        content = request.content

        # æå–ä¸åŒç±»å‹çš„å†…å®¹
        extracted = {
            'text_length': len(content),
            'word_count': len(content.split()),
            'line_count': content.count('\n') + 1,
            'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
            'sentences': self._extract_sentences(content),
            'key_phrases': self._extract_key_phrases(content),
            'numbers': self._extract_numbers(content),
            'emails': self._extract_emails(content),
            'urls': self._extract_urls(content)
        }

        request.results['content_extraction'] = extracted

        print(f"   ğŸ“ æå–å†…å®¹: {extracted['word_count']} è¯, {extracted['paragraph_count']} æ®µè½")
        return ProcessingResult.CONTINUE

    def _extract_sentences(self, content: str) -> List[str]:
        """æå–å¥å­"""
        sentences = re.split(r'[.!?]+', content)
        return [s.strip() for s in sentences if s.strip()][:10]  # æœ€å¤šè¿”å›10ä¸ªå¥å­

    def _extract_key_phrases(self, content: str) -> List[str]:
        """æå–å…³é”®è¯çŸ­è¯­"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPç®—æ³•ï¼‰
        words = re.findall(r'\b[\u4e00-\u9fa5]+\b', content)  # æå–ä¸­æ–‡è¯æ±‡
        word_freq = {}
        for word in words:
            if len(word) >= 2:  # è‡³å°‘2ä¸ªå­—
                word_freq[word] = word_freq.get(word, 0) + 1

        # è¿”å›é¢‘ç‡æœ€é«˜çš„5ä¸ªè¯
        return sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]

    def _extract_numbers(self, content: str) -> List[str]:
        """æå–æ•°å­—"""
        return re.findall(r'\d+\.?\d*', content)

    def _extract_emails(self, content: str) -> List[str]:
        """æå–é‚®ç®±åœ°å€"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        return re.findall(email_pattern, content)

    def _extract_urls(self, content: str) -> List[str]:
        """æå–URL"""
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        return re.findall(url_pattern, content)


class SentimentAnalysisHandler(DocumentHandler):
    """æƒ…æ„Ÿåˆ†æå¤„ç†å™¨"""

    def __init__(self):
        super().__init__("æƒ…æ„Ÿåˆ†æ")
        self.positive_words = ['å¥½', 'ä¼˜ç§€', 'æˆåŠŸ', 'æ»¡æ„', 'å–œæ¬¢', 'æ£’', 'å®Œç¾']
        self.negative_words = ['å·®', 'å¤±è´¥', 'ç³Ÿç³•', 'ä¸æ»¡', 'è®¨åŒ', 'ç³Ÿç³•', 'é”™è¯¯']

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        content = request.content.lower()

        positive_count = sum(1 for word in self.positive_words if word in content)
        negative_count = sum(1 for word in self.negative_words if word in content)

        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            sentiment = "neutral"
            confidence = 0.5
        else:
            sentiment_score = (positive_count - negative_count) / total_sentiment_words
            if sentiment_score > 0.2:
                sentiment = "positive"
            elif sentiment_score < -0.2:
                sentiment = "negative"
            else:
                sentiment = "neutral"
            confidence = min(abs(sentiment_score) + 0.5, 1.0)

        request.results['sentiment_analysis'] = {
            'sentiment': sentiment,
            'confidence': confidence,
            'positive_words': positive_count,
            'negative_words': negative_count,
            'analysis_details': {
                'positive_words_found': [word for word in self.positive_words if word in content],
                'negative_words_found': [word for word in self.negative_words if word in content]
            }
        }

        print(f"   ğŸ˜Š æƒ…æ„Ÿåˆ†æ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.2f})")
        return ProcessingResult.CONTINUE


class AISummaryHandler(DocumentHandler):
    """ğŸ¤– AIæ‘˜è¦å¤„ç†å™¨ - è´£ä»»é“¾ä¸­çš„AIå¢å¼ºèŠ‚ç‚¹

    è´£ä»»é“¾æ¨¡å¼åº”ç”¨ï¼š
    1. AIèƒ½åŠ›é›†æˆï¼šå°†LLMæ‘˜è¦ç”Ÿæˆèƒ½åŠ›é›†æˆåˆ°å¤„ç†é“¾ä¸­
    2. æ™ºèƒ½åˆ†æï¼šåˆ©ç”¨AIç†è§£æ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦
    3. é™çº§æœºåˆ¶ï¼šAIæœåŠ¡ä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§åˆ°è§„åˆ™æ–¹æ³•
    4. ç»“æœæ ‡å‡†åŒ–ï¼šä¸ºåç»­å¤„ç†å™¨æä¾›ç»“æ„åŒ–çš„AIåˆ†æç»“æœ

    æŠ€æœ¯ç‰¹ç‚¹ï¼š
    â€¢ DeepSeek APIé›†æˆ
    â€¢ æ™ºèƒ½é™çº§å¤„ç†
    â€¢ è¯¦ç»†è°ƒç”¨ç»Ÿè®¡
    â€¢ JSONæ ¼å¼ç»“æœè¾“å‡º
    """

    def __init__(self, max_sentences: int = 3):
        super().__init__("AIæ‘˜è¦")
        self.max_sentences = max_sentences
        self.ai_call_count = 0
        self.fallback_count = 0

        print(f"ğŸ¤– åˆå§‹åŒ–{self.name}å¤„ç†å™¨")
        print(f"   ğŸ¯ æœ€å¤§æ‘˜è¦å¥æ•°: {self.max_sentences}")
        print(f"   ğŸ”— AIæœåŠ¡çŠ¶æ€: å·²è¿æ¥")

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        """ğŸ§  æ‰§è¡ŒAIæ‘˜è¦ç”Ÿæˆ - è´£ä»»é“¾ä¸­çš„æ™ºèƒ½å¤„ç†æ­¥éª¤"""
        content = request.content

        print(f"   ğŸ¤– å¼€å§‹AIæ‘˜è¦ç”Ÿæˆ...")
        print(f"   ğŸ“ æ–‡æ¡£é•¿åº¦: {len(content):,} å­—ç¬¦")
        print(f"   ğŸ¯ ç›®æ ‡æ‘˜è¦å¥æ•°: {self.max_sentences}")

        ai_results = {
            'timestamp': datetime.now().isoformat(),
            'processing_status': 'started',
            'ai_service_info': {
                'provider': 'deepseek',
                'model': 'deepseek-chat',
                'max_length': 300
            },
            'performance_metrics': {
                'start_time': time.time(),
                'ai_call_time': 0,
                'total_processing_time': 0
            },
            'content_analysis': {
                'original_length': len(content),
                'word_count': len(content.split()),
                'paragraph_count': len([p for p in content.split('\n\n') if p.strip()])
            },
            'fallback_used': False,
            'errors': []
        }

        try:
            # æ­¥éª¤1: å°è¯•ä½¿ç”¨AIæœåŠ¡ç”Ÿæˆæ‘˜è¦
            print(f"   ğŸ”— å°è¯•è°ƒç”¨DeepSeek AIæœåŠ¡...")
            ai_call_start = time.time()

            summary = self._generate_summary(content)
            ai_call_time = time.time() - ai_call_start

            self.ai_call_count += 1
            ai_results['performance_metrics']['ai_call_time'] = ai_call_time
            ai_results['performance_metrics']['ai_success'] = True

            print(f"   âœ… AIæ‘˜è¦ç”ŸæˆæˆåŠŸ!")
            print(f"   â±ï¸  AIè°ƒç”¨è€—æ—¶: {ai_call_time:.3f}ç§’")

        except Exception as e:
            # æ­¥éª¤2: AIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨é™çº§æ–¹æ³•
            error_msg = f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"
            ai_results['errors'].append(error_msg)
            ai_results['fallback_used'] = True

            print(f"   âš ï¸  AIæœåŠ¡è°ƒç”¨å¤±è´¥ï¼Œå¯ç”¨é™çº§å¤„ç†")
            print(f"   ğŸ“ é”™è¯¯ä¿¡æ¯: {str(e)}")

            fallback_start = time.time()
            summary = self._fallback_summary(content)
            fallback_time = time.time() - fallback_start

            self.fallback_count += 1
            ai_results['performance_metrics']['fallback_time'] = fallback_time
            ai_results['performance_metrics']['ai_success'] = False

            print(f"   âœ… é™çº§æ‘˜è¦ç”Ÿæˆå®Œæˆ!")
            print(f"   â±ï¸  é™çº§å¤„ç†è€—æ—¶: {fallback_time:.3f}ç§’")

        # æ­¥éª¤3: æå–å…³é”®ç‚¹
        print(f"   ğŸ” æå–æ–‡æ¡£å…³é”®ç‚¹...")
        key_points = self._extract_key_points(content)

        # æ­¥éª¤4: è®¡ç®—å‹ç¼©ç‡å’Œå…¶ä»–ç»Ÿè®¡ä¿¡æ¯
        compression_ratio = len(summary) / len(content) if content else 0
        total_processing_time = time.time() - ai_results['performance_metrics']['start_time']

        # å®ŒæˆAIåˆ†æç»“æœ
        ai_results.update({
            'processing_status': 'completed',
            'summary_result': {
                'summary_text': summary,
                'summary_length': len(summary),
                'key_points_count': len(key_points),
                'key_points': key_points,
                'compression_ratio': compression_ratio,
                'compression_percentage': round((1 - compression_ratio) * 100, 2)
            },
            'performance_metrics': {
                **ai_results['performance_metrics'],
                'total_processing_time': total_processing_time,
                'processing_speed': len(content) / total_processing_time if total_processing_time > 0 else 0
            },
            'quality_metrics': {
                'has_summary': len(summary.strip()) > 0,
                'has_key_points': len(key_points) > 0,
                'summary_completeness': min(len(summary.split()) / 50, 1.0),  # å‡è®¾50è¯ä¸ºå®Œæ•´æ‘˜è¦
                'key_points_relevance': len(key_points) / 5.0  # å‡è®¾5ä¸ªå…³é”®ç‚¹ä¸ºæ»¡åˆ†
            },
            'service_statistics': {
                'total_ai_calls': self.ai_call_count,
                'total_fallbacks': self.fallback_count,
                'success_rate': (self.ai_call_count - self.fallback_count) / max(self.ai_call_count, 1) * 100
            }
        })

        # ä¿å­˜AIåˆ†æç»“æœåˆ°è¯·æ±‚å¯¹è±¡
        request.add_result('ai_summary', ai_results)

        # æ‰“å°è¯¦ç»†çš„AIå¤„ç†ç»“æœ
        print(f"   ğŸ¯ AIæ‘˜è¦å¤„ç†å®Œæˆ!")
        print(f"   ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"      â€¢ æ‘˜è¦é•¿åº¦: {ai_results['summary_result']['summary_length']:,} å­—ç¬¦")
        print(f"      â€¢ å…³é”®ç‚¹æ•°: {ai_results['summary_result']['key_points_count']} ä¸ª")
        print(f"      â€¢ å‹ç¼©ç‡: {compression_ratio:.2%}")
        print(f"      â€¢ å‹ç¼©é‡: {ai_results['summary_result']['compression_percentage']:.1f}%")
        print(f"      â€¢ å¤„ç†é€Ÿåº¦: {ai_results['performance_metrics']['processing_speed']:.0f} å­—ç¬¦/ç§’")

        print(f"   ğŸ¨ æ‘˜è¦å†…å®¹é¢„è§ˆ:")
        print(f"      {summary[:150]}{'...' if len(summary) > 150 else ''}")

        if key_points:
            print(f"   ğŸ”‘ å…³é”®ç‚¹é¢„è§ˆ:")
            for i, point in enumerate(key_points[:3], 1):
                print(f"      {i}. {point[:80]}{'...' if len(point) > 80 else ''}")
            if len(key_points) > 3:
                print(f"      ... è¿˜æœ‰ {len(key_points) - 3} ä¸ªå…³é”®ç‚¹")

        print(f"   ğŸ“ˆ AIæœåŠ¡ç»Ÿè®¡:")
        print(f"      â€¢ AIè°ƒç”¨æ¬¡æ•°: {ai_results['service_statistics']['total_ai_calls']}")
        print(f"      â€¢ é™çº§ä½¿ç”¨æ¬¡æ•°: {ai_results['service_statistics']['total_fallbacks']}")
        print(f"      â€¢ æˆåŠŸç‡: {ai_results['service_statistics']['success_rate']:.1f}%")

        # æ‰“å°JSONæ ¼å¼çš„è¯¦ç»†AIå¤„ç†ç»“æœ
        print(f"   ğŸ“„ è¯¦ç»†AIå¤„ç†ç»“æœ (JSON):")
        ai_json = json.dumps(ai_results, ensure_ascii=False, indent=8)
        print(f"      {ai_json}")

        return ProcessingResult.CONTINUE

    def _generate_summary(self, content: str) -> str:
        """ğŸ”— ä½¿ç”¨DeepSeek AIç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        try:
            print(f"      ğŸš€ è°ƒç”¨DeepSeek API...")
            ai_service = get_ai_service("deepseek")

            # æ„å»ºä¸“é—¨çš„æ‘˜è¦è¯·æ±‚prompt
            summary_prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç®€æ´å‡†ç¡®çš„æ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. çªå‡ºä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒä¿¡æ¯
2. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œè¯­è¨€æµç•…
3. æ§åˆ¶åœ¨{self.max_sentences * 30}å­—ä»¥å†…
4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º

æ–‡æ¡£å†…å®¹ï¼š
{content}
"""

            result = ai_service.extract_summary(content, max_length=300)

            if result["success"]:
                print(f"      âœ… DeepSeek APIè°ƒç”¨æˆåŠŸ")
                return result["content"]
            else:
                print(f"      âŒ DeepSeek APIè¿”å›å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                raise Exception(f"AI APIè¿”å›é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        except Exception as e:
            print(f"      ğŸ’¥ DeepSeek APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            raise Exception(f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")

    def _fallback_summary(self, content: str) -> str:
        """é™çº§æ‘˜è¦ç”Ÿæˆæ–¹æ³•"""
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= self.max_sentences:
            return '. '.join(sentences)

        # ç®€å•çš„æ‘˜è¦ç­–ç•¥ï¼šé€‰æ‹©å‰é¢å’Œåé¢çš„å¥å­
        selected_sentences = sentences[:self.max_sentences//2] + sentences[-self.max_sentences//2:]

        return '. '.join(selected_sentences) + '.'

    def _extract_key_points(self, content: str) -> List[str]:
        """æå–å…³é”®ç‚¹"""
        # ç®€å•çš„å…³é”®ç‚¹æå–ç­–ç•¥
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        # é€‰æ‹©åŒ…å«æ•°å­—æˆ–é‡è¦è¯æ±‡çš„å¥å­ä½œä¸ºå…³é”®ç‚¹
        important_keywords = ['é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'ä¸»è¦', 'å¿…é¡»', 'åº”è¯¥', 'éœ€è¦']

        key_points = []
        for sentence in sentences:
            if (any(keyword in sentence for keyword in important_keywords) or
                re.search(r'\d+', sentence)):  # åŒ…å«æ•°å­—
                key_points.append(sentence[:100] + '...' if len(sentence) > 100 else sentence)
                if len(key_points) >= 5:
                    break

        return key_points or sentences[:3]  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ç‚¹ï¼Œè¿”å›å‰3ä¸ªå¥å­


class QualityCheckHandler(DocumentHandler):
    """è´¨é‡æ£€æŸ¥å¤„ç†å™¨"""

    def __init__(self, min_quality_score: float = 0.6):
        super().__init__("è´¨é‡æ£€æŸ¥")
        self.min_quality_score = min_quality_score

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        content = request.content
        metadata = request.metadata

        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = self._calculate_quality_score(content, metadata)

        quality_issues = []
        if quality_score < self.min_quality_score:
            quality_issues = self._identify_quality_issues(content)

        request.results['quality_check'] = {
            'score': quality_score,
            'passed': quality_score >= self.min_quality_score,
            'issues': quality_issues,
            'recommendations': self._get_recommendations(quality_score, quality_issues)
        }

        if quality_score < self.min_quality_score:
            print(f"   âš ï¸  è´¨é‡æ£€æŸ¥æœªé€šè¿‡: {quality_score:.2f} < {self.min_quality_score}")
            # å¯ä»¥é€‰æ‹©ç»§ç»­å¤„ç†æˆ–åœæ­¢
            return ProcessingResult.CONTINUE
        else:
            print(f"   âœ… è´¨é‡æ£€æŸ¥é€šè¿‡: {quality_score:.2f}")
            return ProcessingResult.CONTINUE

    def _calculate_quality_score(self, content: str, metadata: Dict[str, Any]) -> float:
        """è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°"""
        score = 0.0

        # é•¿åº¦åˆ†æ•° (0-0.3)
        word_count = len(content.split())
        if 50 <= word_count <= 1000:
            score += 0.3
        elif word_count > 1000:
            score += 0.2
        else:
            score += 0.1

        # ç»“æ„åˆ†æ•° (0-0.3)
        if content.count('\n\n') > 0:  # æœ‰æ®µè½
            score += 0.2
        if any(header in content for header in ['#', '##', '###']):  # æœ‰æ ‡é¢˜
            score += 0.1

        # å†…å®¹å®Œæ•´æ€§ (0-0.2)
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', content)
        if len(sentences) > 1:
            score += 0.2

        # æ ¼å¼æ­£ç¡®æ€§ (0-0.2)
        if metadata.get('format') in ['text', 'markdown', 'html']:
            score += 0.2

        return min(score, 1.0)

    def _identify_quality_issues(self, content: str) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""
        issues = []

        if len(content.strip()) < 50:
            issues.append("å†…å®¹è¿‡çŸ­")

        if len(content.split()) < 10:
            issues.append("è¯æ±‡é‡ä¸è¶³")

        if not re.search(r'[.!?ã€‚ï¼ï¼Ÿ]+', content):
            issues.append("ç¼ºå°‘å¥å­æ ‡ç‚¹")

        if content.count('\n') == 0 and len(content) > 200:
            issues.append("ç¼ºå°‘åˆ†æ®µ")

        return issues

    def _get_recommendations(self, score: float, issues: List[str]) -> List[str]:
        """è·å–æ”¹è¿›å»ºè®®"""
        recommendations = []

        if score < 0.5:
            recommendations.append("å»ºè®®å¤§å¹…é‡å†™ï¼Œå¢åŠ å†…å®¹æ·±åº¦")
        elif score < 0.7:
            recommendations.append("å»ºè®®å¢åŠ æ›´å¤šç»†èŠ‚å’Œç»“æ„")

        if "å†…å®¹è¿‡çŸ­" in issues:
            recommendations.append("å¢åŠ æ›´å¤šå†…å®¹")

        if "ç¼ºå°‘åˆ†æ®µ" in issues:
            recommendations.append("æ·»åŠ é€‚å½“çš„æ®µè½åˆ†éš”")

        if "ç¼ºå°‘å¥å­æ ‡ç‚¹" in issues:
            recommendations.append("æ·»åŠ æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·")

        return recommendations


class OutputFormatterHandler(DocumentHandler):
    """è¾“å‡ºæ ¼å¼åŒ–å¤„ç†å™¨"""

    def __init__(self, output_format: str = 'json'):
        super().__init__("è¾“å‡ºæ ¼å¼åŒ–")
        self.output_format = output_format

    def handle(self, request: ProcessingRequest) -> ProcessingResult:
        results = request.results

        # æ·»åŠ å¤„ç†å®Œæˆæ—¶é—´æˆ³
        results['processed_at'] = self._get_timestamp()
        results['processor_chain'] = self._get_processing_chain(request)

        # æ ¼å¼åŒ–è¾“å‡º
        formatted_output = self._format_output(results)

        request.results['final_output'] = formatted_output

        print(f"   ğŸ“„ è¾“å‡ºæ ¼å¼åŒ–å®Œæˆ: {self.output_format}")
        return ProcessingResult.STOP  # è¿™æ˜¯æœ€åä¸€ä¸ªå¤„ç†å™¨ï¼Œæ‰€ä»¥åœæ­¢

    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def _get_processing_chain(self, request: ProcessingRequest) -> List[str]:
        """è·å–å¤„ç†é“¾ä¿¡æ¯"""
        # è¿™é‡Œåº”è¯¥è®°å½•å®é™…ç»è¿‡çš„å¤„ç†å™¨
        return ["æ ¼å¼éªŒè¯", "å†…å®¹æå–", "æƒ…æ„Ÿåˆ†æ", "AIæ‘˜è¦", "è´¨é‡æ£€æŸ¥", "è¾“å‡ºæ ¼å¼åŒ–"]

    def _format_output(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
        formatted = {
            'status': 'success' if 'error' not in results else 'error',
            'summary': {
                'processing_time': results.get('processed_at'),
                'content_length': results.get('content_extraction', {}).get('text_length', 0),
                'sentiment': results.get('sentiment_analysis', {}).get('sentiment', 'unknown'),
                'quality_score': results.get('quality_check', {}).get('score', 0)
            },
            'detailed_results': results
        }

        if 'error' in results:
            formatted['error'] = results['error']

        return formatted


# å¤„ç†é“¾æ„å»ºå™¨
class ProcessingChainBuilder:
    """å¤„ç†é“¾æ„å»ºå™¨"""

    def __init__(self):
        self.handlers = []

    def add_handler(self, handler: DocumentHandler) -> 'ProcessingChainBuilder':
        """æ·»åŠ å¤„ç†å™¨"""
        self.handlers.append(handler)
        return self

    def build(self) -> DocumentHandler:
        """æ„å»ºå¤„ç†é“¾"""
        if not self.handlers:
            raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªå¤„ç†å™¨")

        # è¿æ¥æ‰€æœ‰å¤„ç†å™¨
        for i in range(len(self.handlers) - 1):
            self.handlers[i].set_next(self.handlers[i + 1])

        return self.handlers[0]

    def create_default_chain(self) -> DocumentHandler:
        """åˆ›å»ºé»˜è®¤çš„å¤„ç†é“¾"""
        return (
            self.add_handler(FormatValidationHandler())
            .add_handler(ContentExtractionHandler())
            .add_handler(SentimentAnalysisHandler())
            .add_handler(AISummaryHandler())
            .add_handler(QualityCheckHandler())
            .add_handler(OutputFormatterHandler())
            .build()
        )


# å¢å¼ºç‰ˆç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("ğŸ”— è´£ä»»é“¾æ¨¡å¼ - å¢å¼ºç‰ˆæ¼”ç¤º")
    print("="*80)
    print("ğŸ¯ æ¼”ç¤ºç›®æ ‡ï¼šå±•ç¤ºå¢å¼ºçš„è´£ä»»é“¾å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬AIé›†æˆã€è¯¦ç»†ç»Ÿè®¡å’ŒJSONè¾“å‡º")
    print("="*80)

    # åˆ›å»ºå¤„ç†é“¾
    print("\nğŸ—ï¸  æ„å»ºå¤„ç†é“¾...")
    chain_builder = ProcessingChainBuilder()
    processing_chain = chain_builder.create_default_chain()

    # æµ‹è¯•æ–‡æ¡£ - æ›´ä¸°å¯Œçš„å†…å®¹ç”¨äºå±•ç¤ºå„ç§å¤„ç†åŠŸèƒ½
    test_content = """
    # AIé©±åŠ¨çš„æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿé¡¹ç›®æ€»ç»“

    ## é¡¹ç›®æ¦‚è¿°

    æœ¬é¡¹ç›®åœ¨2024å¹´å–å¾—äº†éå¸¸æ˜¾è‘—çš„æˆæœï¼æˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿè¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸå®Œæˆäº†æ‰€æœ‰é¢„å®šç›®æ ‡ã€‚
    è¿™æ˜¯ä¸€ä¸ªç»“åˆäº†äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆ›æ–°é¡¹ç›®ï¼Œæ—¨åœ¨æä¾›æ™ºèƒ½åŒ–çš„æ–‡æ¡£å¤„ç†è§£å†³æ–¹æ¡ˆã€‚

    ## æ ¸å¿ƒæˆå°±

    æˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š
    1. **æ™ºèƒ½ç”¨æˆ·ç®¡ç†æ¨¡å—** - åŸºäºæœºå™¨å­¦ä¹ çš„ç”¨æˆ·è¡Œä¸ºåˆ†æ
    2. **é«˜æ€§èƒ½æ•°æ®å¤„ç†å¼•æ“** - æ”¯æŒTBçº§æ•°æ®å®æ—¶å¤„ç†
    3. **AIå¢å¼ºçš„æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ** - è‡ªåŠ¨åŒ–ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š

    ## å…³é”®æ•°æ®æŒ‡æ ‡

    ğŸ“ˆ **æ€§èƒ½æå‡æ•°æ®**ï¼š
    - ç”¨æˆ·æ»¡æ„åº¦ä»75%æå‡è‡³95%ï¼Œæå‡äº†20ä¸ªç™¾åˆ†ç‚¹
    - ç³»ç»Ÿæ€§èƒ½æå‡50%ï¼Œå“åº”æ—¶é—´ä»200msé™è‡³100ms
    - è¿è¥æˆæœ¬é™ä½30%ï¼Œæ¯å¹´èŠ‚çœçº¦500ä¸‡å…ƒ

    ğŸ” **æŠ€æœ¯æŒ‡æ ‡**ï¼š
    - ç³»ç»Ÿå¯ç”¨æ€§è¾¾åˆ°99.9%
    - æ—¥å¤„ç†æ–‡æ¡£æ•°é‡ï¼š100ä¸‡+ä»½
    - APIè°ƒç”¨é‡ï¼šæ—¥å‡5000ä¸‡æ¬¡
    - æ•°æ®å¤„ç†å‡†ç¡®ç‡ï¼š98.7%

    ## å›¢é˜Ÿåä½œ

    è¿™ä¸ªé¡¹ç›®çš„æˆåŠŸç¦»ä¸å¼€å›¢é˜Ÿçš„ç´§å¯†åˆä½œï¼æˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿã€äº§å“å›¢é˜Ÿå’ŒAIç ”ç©¶å›¢é˜Ÿå…±åŒåŠªåŠ›ï¼Œ
    å…‹æœäº†é‡é‡æŠ€æœ¯æŒ‘æˆ˜ï¼Œæœ€ç»ˆäº¤ä»˜äº†è¿™ä¸ªå“è¶Šçš„äº§å“ã€‚

    ## è”ç³»æ–¹å¼

    - é¡¹ç›®è´Ÿè´£äººï¼šå¼ æ˜åšå£«
    - é‚®ç®±ï¼šzhangming@aidocs.example.com
    - æŠ€æœ¯æ”¯æŒï¼štech-support@aidocs.example.com
    - å®˜æ–¹ç½‘ç«™ï¼šhttps://aidocs.example.com/smart-processor

    ## ç»“è®º

    æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªAIé©±åŠ¨çš„æ–‡æ¡£å¤„ç†ç³»ç»Ÿé¡¹ç›®å–å¾—äº†å·¨å¤§æˆåŠŸï¼å®ƒä¸ä»…æå‡äº†æˆ‘ä»¬çš„æŠ€æœ¯å®åŠ›ï¼Œ
    ä¹Ÿä¸ºå®¢æˆ·åˆ›é€ äº†æ˜¾è‘—çš„ä»·å€¼ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªç³»ç»Ÿå°†åœ¨æœªæ¥çš„æ–‡æ¡£å¤„ç†é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
    """

    # åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†è¯·æ±‚
    request = ProcessingRequest(
        content=test_content,
        metadata={
            'format': 'markdown',
            'author': 'å¼ æ˜åšå£«',
            'created_date': '2024-01-15',
            'project_id': 'AI_DOCS_2024_001',
            'department': 'AIç ”å‘éƒ¨',
            'priority': 'high',
            'tags': ['AI', 'æ–‡æ¡£å¤„ç†', 'é¡¹ç›®æ€»ç»“', 'æŠ€æœ¯åˆ›æ–°']
        }
    )

    print("\nğŸ“„ å¤„ç†æ–‡æ¡£ä¿¡æ¯:")
    print("-" * 60)
    print(f"ğŸ“ æ–‡æ¡£æ ‡é¢˜: AIé©±åŠ¨çš„æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿé¡¹ç›®æ€»ç»“")
    print(f"ğŸ‘¤ ä½œè€…: {request.metadata['author']}")
    print(f"ğŸ“… åˆ›å»ºæ—¥æœŸ: {request.metadata['created_date']}")
    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(test_content):,} å­—ç¬¦")
    print(f"ğŸ”¢ è¯æ±‡æ•°é‡: {len(test_content.split()):,} è¯")
    print(f"ğŸ“‘ æ®µè½æ•°é‡: {len([p for p in test_content.split('\n\n') if p.strip()])} ä¸ª")
    print(f"ğŸ·ï¸  æ ‡ç­¾: {', '.join(request.metadata['tags'])}")
    print("-" * 60)

    print("\nğŸ“„ æ–‡æ¡£å†…å®¹é¢„è§ˆ:")
    print("-" * 60)
    print(test_content[:300] + "...")
    print("-" * 60)

    # æ‰§è¡Œå¢å¼ºç‰ˆå¤„ç†é“¾
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œè´£ä»»é“¾å¤„ç†...")
    print("="*60)

    chain_start_time = time.time()
    result = processing_chain.process(request)
    total_chain_time = time.time() - chain_start_time

    print("\n" + "="*60)
    print("ğŸ“Š è´£ä»»é“¾å¤„ç†å®Œæˆ - ç»¼åˆç»Ÿè®¡æŠ¥å‘Š")
    print("="*60)

    # æ˜¾ç¤ºæ€»ä½“å¤„ç†ç»Ÿè®¡
    print(f"\nâ±ï¸  å¤„ç†é“¾æ€»è€—æ—¶: {total_chain_time:.3f}ç§’")
    print(f"ğŸ“‹ ç»è¿‡çš„å¤„ç†å™¨æ•°é‡: {len(request.processing_stats)}")
    print(f"âœ… å¤„ç†çŠ¶æ€: {'å¤±è´¥' if 'error' in request.results else 'æˆåŠŸ'}")

    # æ˜¾ç¤ºå„å¤„ç†å™¨çš„è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“ˆ å„å¤„ç†å™¨è¯¦ç»†ç»Ÿè®¡:")
    print("-" * 60)
    total_processing_time = 0
    for handler_name, stats in request.processing_stats.items():
        processing_time = stats.get('processing_time', 0)
        total_processing_time += processing_time
        status = stats.get('status', 'unknown')
        print(f"ğŸ”§ {handler_name}:")
        print(f"   â€¢ å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
        print(f"   â€¢ çŠ¶æ€: {status}")
        print(f"   â€¢ å¤„ç†å™¨ID: {stats.get('handler_id', 'N/A')}")
        if 'error' in stats:
            print(f"   â€¢ é”™è¯¯: {stats['error']}")

    print(f"\nğŸ“Š å¤„ç†æ•ˆç‡åˆ†æ:")
    print(f"   â€¢ æ€»å¤„ç†æ—¶é—´: {total_processing_time:.3f}ç§’")
    print(f"   â€¢ å¹³å‡æ¯å¤„ç†å™¨è€—æ—¶: {total_processing_time / max(len(request.processing_stats), 1):.3f}ç§’")
    print(f"   â€¢ æ–‡æ¡£å¤„ç†é€Ÿåº¦: {len(test_content) / total_chain_time:.0f} å­—ç¬¦/ç§’")

    # æ˜¾ç¤ºæœ€ç»ˆå¤„ç†ç»“æœ
    print(f"\nğŸ“‹ æœ€ç»ˆå¤„ç†ç»“æœæ‘˜è¦:")
    print("-" * 60)

    if 'format_validation' in request.results:
        validation = request.results['format_validation']['summary']
        print(f"âœ… æ ¼å¼éªŒè¯: {validation['format']}æ ¼å¼ï¼Œ{validation['size']:,}å­—ç¬¦ï¼Œé€šè¿‡")

    if 'content_extraction' in request.results:
        extraction = request.results['content_extraction']
        print(f"ğŸ“ å†…å®¹æå–: {extraction['word_count']}è¯ï¼Œ{extraction['paragraph_count']}æ®µè½")

    if 'sentiment_analysis' in request.results:
        sentiment = request.results['sentiment_analysis']
        print(f"ğŸ˜Š æƒ…æ„Ÿåˆ†æ: {sentiment['sentiment']} (ç½®ä¿¡åº¦: {sentiment['confidence']:.2f})")

    if 'ai_summary' in request.results:
        ai_result = request.results['ai_summary']
        summary = ai_result['summary_result']
        print(f"ğŸ¤– AIæ‘˜è¦: {summary['summary_length']}å­—ç¬¦ï¼Œå‹ç¼©ç‡{summary['compression_ratio']:.2%}")
        print(f"   ğŸ“ˆ AIè°ƒç”¨æˆåŠŸç‡: {ai_result['service_statistics']['success_rate']:.1f}%")

    if 'quality_check' in request.results:
        quality = request.results['quality_check']
        print(f"ğŸ” è´¨é‡æ£€æŸ¥: {quality['score']:.2f}åˆ† ({'é€šè¿‡' if quality['passed'] else 'æœªé€šè¿‡'})")

    # æ˜¾ç¤ºæœ€ç»ˆJSONè¾“å‡º - ä¿®å¤å¾ªç¯å¼•ç”¨é—®é¢˜
    print(f"\nğŸ“„ å®Œæ•´å¤„ç†ç»“æœ (JSONæ ¼å¼):")
    print("="*60)
    final_output = request.results.get('final_output', {})
    if final_output:
        try:
            # åˆ›å»ºå®‰å…¨çš„JSONè¾“å‡ºï¼Œç§»é™¤å¯èƒ½å¯¼è‡´å¾ªç¯å¼•ç”¨çš„å¯¹è±¡
            safe_output = create_safe_json_output(final_output)
            print(json.dumps(safe_output, ensure_ascii=False, indent=4))
        except (ValueError, TypeError) as e:
            print(f"âš ï¸  JSONåºåˆ—åŒ–å¤±è´¥: {str(e)}")
            print("ğŸ“„ è¾“å‡ºç®€åŒ–ç‰ˆæœ¬:")
            simplified_output = {
                "status": final_output.get('status', 'unknown'),
                "summary": final_output.get('summary', {}),
                "timestamp": datetime.now().isoformat()
            }
            print(json.dumps(simplified_output, ensure_ascii=False, indent=2))
    else:
        print("âš ï¸  æœ€ç»ˆè¾“å‡ºä¸ºç©ºï¼Œå¯èƒ½å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")

    print("\n" + "="*60)
    print("ğŸ‰ è´£ä»»é“¾æ¨¡å¼å¢å¼ºç‰ˆæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ ä¸»è¦å¢å¼ºç‰¹æ€§:")
    print("   â€¢ è¯¦ç»†çš„å¤„ç†æµç¨‹è¿½è¸ªå’Œç»Ÿè®¡")
    print("   â€¢ AIæœåŠ¡é›†æˆä¸æ™ºèƒ½é™çº§æœºåˆ¶")
    print("   â€¢ JSONæ ¼å¼çš„ç»“æ„åŒ–è¾“å‡º")
    print("   â€¢ å…¨é¢çš„é”™è¯¯å¤„ç†å’Œæ€§èƒ½ç›‘æ§")
    print("   â€¢ å¯æ‰©å±•çš„å¤„ç†å™¨æ¶æ„")
    print("="*60)