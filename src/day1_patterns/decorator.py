"""
è£…é¥°å™¨æ¨¡å¼ - æ–‡æ¡£å¤„ç†åŠŸèƒ½å¢å¼º

è£…é¥°å™¨æ¨¡å¼å…è®¸åœ¨ä¸ä¿®æ”¹åŸå§‹å¯¹è±¡çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€åœ°æ·»åŠ æ–°åŠŸèƒ½ã€‚
åœ¨AIå·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è£…é¥°å™¨ä¸ºæ–‡æ¡£å¤„ç†æ·»åŠ æ—¥å¿—ã€ç¼“å­˜ã€
æ€§èƒ½ç›‘æ§ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½ã€‚
"""

import time
import json
import hashlib
from functools import wraps
from typing import Dict, Any, List, Callable
from dataclasses import dataclass
import logging
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ai_service import get_ai_service

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingContext:
    """å¤„ç†ä¸Šä¸‹æ–‡"""
    document_id: str
    user_id: str = "default"
    session_id: str = ""
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DocumentProcessor:
    """
    åŸºç¡€æ–‡æ¡£å¤„ç†å™¨ - è£…é¥°å™¨æ¨¡å¼çš„"ç»„ä»¶"ç±»

    è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„æ ¸å¿ƒï¼Œå®šä¹‰äº†è¢«è£…é¥°çš„åŸºæœ¬å¯¹è±¡ã€‚
    æ‰€æœ‰è£…é¥°å™¨éƒ½å°†åŸºäºè¿™ä¸ªåŸºç¡€å¤„ç†å™¨æ¥å¢å¼ºåŠŸèƒ½ã€‚
    """

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        åŸºç¡€å¤„ç†æ–¹æ³• - è£…é¥°å™¨é“¾çš„æœ€ç»ˆæ‰§è¡Œç‚¹

        è¿™ä¸ªæ–¹æ³•æ˜¯æ‰€æœ‰è£…é¥°å™¨è°ƒç”¨çš„æœ€ç»ˆç›®æ ‡ã€‚
        è£…é¥°å™¨é“¾ä¼šåœ¨è°ƒç”¨è¿™ä¸ªæ–¹æ³•ä¹‹å‰æˆ–ä¹‹åæ·»åŠ é¢å¤–åŠŸèƒ½ã€‚

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: åŸºç¡€å¤„ç†ç»“æœ
        """
        print(f"ğŸ”§ [åŸºç¡€å¤„ç†å™¨] å¼€å§‹æ‰§è¡ŒåŸºç¡€å¤„ç†é€»è¾‘")
        print(f"ğŸ“„ [æ–‡æ¡£ä¿¡æ¯] ID: {context.document_id}, ç”¨æˆ·: {context.user_id}")
        print(f"ğŸ“ [å†…å®¹é•¿åº¦] {len(content)} å­—ç¬¦")
        print(f"â° [å¤„ç†æ—¶é—´] {time.time():.2f}")

        # æ¨¡æ‹ŸåŸºç¡€å¤„ç†é€»è¾‘
        base_result = {
            "content": content,
            "processed_at": time.time(),
            "context": context,
            "processor_type": "base",
            "processing_stage": "base_completed"
        }

        print(f"âœ… [åŸºç¡€å¤„ç†å®Œæˆ] è¿”å›åŸºç¡€å¤„ç†ç»“æœ")
        return base_result

    def batch_process(self, contents: List[str], context: ProcessingContext) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ–¹æ³•"""
        results = []
        for content in contents:
            result = self.process(content, context)
            results.append(result)
        return results


class ProcessorDecorator:
    """
    æ–‡æ¡£å¤„ç†å™¨è£…é¥°å™¨åŸºç±» - è£…é¥°å™¨æ¨¡å¼çš„"è£…é¥°å™¨"æŠ½è±¡ç±»

    è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„æŠ½è±¡ç»„ä»¶ï¼Œå®šä¹‰äº†æ‰€æœ‰è£…é¥°å™¨çš„é€šç”¨æ¥å£ã€‚
    æ‰€æœ‰å…·ä½“è£…é¥°å™¨éƒ½å¿…é¡»ç»§æ‰¿è¿™ä¸ªåŸºç±»å¹¶å®ç°ç›¸åº”çš„å¢å¼ºåŠŸèƒ½ã€‚

    è®¾è®¡æ¨¡å¼è§’è‰²ï¼š
    - Decorator (è£…é¥°å™¨æŠ½è±¡ç±»): å®šä¹‰è£…é¥°å™¨æ¥å£ï¼ŒæŒæœ‰ç»„ä»¶å¼•ç”¨
    - ç»´æŠ¤ä¸€ä¸ªæŒ‡å‘Componentå¯¹è±¡çš„å¼•ç”¨
    - å®šä¹‰ä¸€ä¸ªä¸Componentæ¥å£ä¸€è‡´çš„æ¥å£
    """

    def __init__(self, processor: DocumentProcessor):
        """
        åˆå§‹åŒ–è£…é¥°å™¨

        Args:
            processor: è¢«è£…é¥°çš„å¤„ç†å™¨å¯¹è±¡ï¼ˆç»„ä»¶ï¼‰
        """
        print(f"ğŸ­ [è£…é¥°å™¨åˆå§‹åŒ–] åˆ›å»ºè£…é¥°å™¨: {self.__class__.__name__}")
        print(f"ğŸ”— [è£…é¥°é“¾] è£…é¥°ç›®æ ‡: {processor.__class__.__name__}")
        self.processor = processor
        print(f"âœ… [è£…é¥°å™¨å°±ç»ª] {self.__class__.__name__} å·²ç»‘å®šåˆ° {processor.__class__.__name__}")

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        å¤„ç†æ–¹æ³• - è£…é¥°å™¨æ¨¡å¼çš„æ ¸å¿ƒå§”æ‰˜æœºåˆ¶

        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†è£…é¥°å™¨æ¨¡å¼çš„å§”æ‰˜æœºåˆ¶ï¼š
        1. è£…é¥°å™¨å¯ä»¥åœ¨è°ƒç”¨è¢«è£…é¥°å¯¹è±¡ä¹‹å‰æ‰§è¡Œå‰ç½®æ“ä½œ
        2. é€šè¿‡self.processor.process()è°ƒç”¨è¢«è£…é¥°å¯¹è±¡
        3. è£…é¥°å™¨å¯ä»¥åœ¨è°ƒç”¨è¢«è£…é¥°å¯¹è±¡ä¹‹åæ‰§è¡Œåç½®æ“ä½œ

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: è£…é¥°åçš„å¤„ç†ç»“æœ
        """
        print(f"ğŸ”„ [è£…é¥°å™¨å§”æ‰˜] {self.__class__.__name__} å§”æ‰˜ç»™ {self.processor.__class__.__name__}")

        # ç›´æ¥å§”æ‰˜ç»™è¢«è£…é¥°çš„å¯¹è±¡ - è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„æ ¸å¿ƒ
        result = self.processor.process(content, context)

        print(f"ğŸ”„ [å§”æ‰˜è¿”å›] {self.processor.__class__.__name__} è¿”å›ç»™ {self.__class__.__name__}")
        return result

    def batch_process(self, contents: List[str], context: ProcessingContext) -> List[Dict[str, Any]]:
        """å§”æ‰˜æ‰¹é‡å¤„ç†"""
        print(f"ğŸ“¦ [æ‰¹é‡å§”æ‰˜] {self.__class__.__name__} æ‰¹é‡å§”æ‰˜ç»™ {self.processor.__class__.__name__}")
        return self.processor.batch_process(contents, context)

    def _print_decorator_info(self, stage: str = "å¤„ç†ä¸­"):
        """
        æ‰“å°è£…é¥°å™¨ä¿¡æ¯ - ç”¨äºè°ƒè¯•å’Œç†è§£è£…é¥°å™¨é“¾

        Args:
            stage: å½“å‰æ‰§è¡Œé˜¶æ®µ
        """
        print(f"ğŸ­ [è£…é¥°å™¨ä¿¡æ¯] ç±»å: {self.__class__.__name__}")
        print(f"ğŸ¯ [æ‰§è¡Œé˜¶æ®µ] {stage}")
        print(f"ğŸ”— [è£…é¥°å¯¹è±¡] {self.processor.__class__.__name__}")

        # æ˜¾ç¤ºè£…é¥°å™¨é“¾ç»“æ„
        current = self
        chain = []
        while hasattr(current, 'processor'):
            chain.append(current.__class__.__name__)
            if hasattr(current.processor, '__class__'):
                if isinstance(current.processor, ProcessorDecorator):
                    current = current.processor
                else:
                    chain.append(current.processor.__class__.__name__)
                    break
            else:
                break

        print(f"ğŸ”— [è£…é¥°å™¨é“¾] {' -> '.join(chain)}")


class LoggingDecorator(ProcessorDecorator):
    """
    æ—¥å¿—è£…é¥°å™¨ - è£…é¥°å™¨æ¨¡å¼çš„å…·ä½“è£…é¥°å™¨å®ç°

    è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„"å…·ä½“è£…é¥°å™¨"ä¹‹ä¸€ï¼Œä¸ºå¤„ç†å™¨æ·»åŠ æ—¥å¿—è®°å½•åŠŸèƒ½ã€‚
    å±•ç¤ºäº†è£…é¥°å™¨å¦‚ä½•åœ¨ä¸ä¿®æ”¹åŸå§‹ä»£ç çš„æƒ…å†µä¸‹æ·»åŠ æ–°åŠŸèƒ½ã€‚

    å¢å¼ºåŠŸèƒ½ï¼š
    - å¤„ç†å‰æ—¥å¿—è®°å½•
    - å¤„ç†æ—¶é—´ç»Ÿè®¡
    - è¯¦ç»†çš„å¤„ç†æ­¥éª¤è·Ÿè¸ª
    - JSONæ ¼å¼çš„æ—¥å¿—è¾“å‡º
    """

    def __init__(self, processor: DocumentProcessor, log_level: int = logging.INFO):
        """
        åˆå§‹åŒ–æ—¥å¿—è£…é¥°å™¨

        Args:
            processor: è¢«è£…é¥°çš„å¤„ç†å™¨
            log_level: æ—¥å¿—çº§åˆ«
        """
        super().__init__(processor)
        self.log_level = log_level
        self.log_history = []  # å­˜å‚¨æ—¥å¿—å†å²

        print(f"ğŸ“ [æ—¥å¿—è£…é¥°å™¨] åˆå§‹åŒ–æ—¥å¿—è®°å½•åŠŸèƒ½")
        print(f"ğŸ“Š [æ—¥å¿—çº§åˆ«] {logging.getLevelName(log_level)}")
        print(f"ğŸ“š [æ—¥å¿—å†å²] åˆå§‹åŒ–æ—¥å¿—å†å²è®°å½•")

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        å¸¦æ—¥å¿—çš„æ–‡æ¡£å¤„ç†æ–¹æ³•

        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†è£…é¥°å™¨æ¨¡å¼çš„å…¸å‹ç»“æ„ï¼š
        1. å‰ç½®å¤„ç†ï¼šè®°å½•å¼€å§‹æ—¥å¿—
        2. è°ƒç”¨è¢«è£…é¥°å¯¹è±¡ï¼šæ‰§è¡Œå®é™…å¤„ç†
        3. åç½®å¤„ç†ï¼šè®°å½•ç»“æŸæ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: æ·»åŠ äº†æ—¥å¿—ä¿¡æ¯çš„å¤„ç†ç»“æœ
        """
        print(f"\nğŸ“ [æ—¥å¿—è£…é¥°å™¨] ================ å¼€å§‹æ‰§è¡Œ ================")
        self._print_decorator_info("æ—¥å¿—è®°å½•é˜¶æ®µ")

        # ğŸ”¥ å‰ç½®å¢å¼ºåŠŸèƒ½ - è®°å½•å¤„ç†å¼€å§‹æ—¥å¿—
        start_log = {
            "timestamp": time.time(),
            "event": "processing_started",
            "document_id": context.document_id,
            "user_id": context.user_id,
            "content_length": len(content),
            "metadata": context.metadata
        }

        print(f"ğŸš€ [å¼€å§‹æ—¥å¿—] è®°å½•å¤„ç†å¼€å§‹:")
        self._print_json_log(start_log, "ğŸ“‹")

        # è®°å½•åˆ°æ—¥å¿—å†å²
        self.log_history.append(start_log)

        # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
        start_time = time.time()
        print(f"â° [æ—¶é—´æˆ³] å¼€å§‹æ—¶é—´: {start_time:.3f}")

        # ğŸ¯ è°ƒç”¨è¢«è£…é¥°å¯¹è±¡ - è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„æ ¸å¿ƒ
        print(f"ğŸ”„ [å§”æ‰˜è°ƒç”¨] è°ƒç”¨è¢«è£…é¥°å¯¹è±¡çš„å¤„ç†æ–¹æ³•...")
        result = super().process(content, context)
        print(f"ğŸ”„ [å§”æ‰˜è¿”å›] è¢«è£…é¥°å¯¹è±¡å¤„ç†å®Œæˆ")

        # è®°å½•å¤„ç†ç»“æŸæ—¶é—´
        end_time = time.time()
        processing_time = end_time - start_time

        # ğŸ”¥ åç½®å¢å¼ºåŠŸèƒ½ - è®°å½•å¤„ç†ç»“æŸæ—¥å¿—
        end_log = {
            "timestamp": end_time,
            "event": "processing_completed",
            "document_id": context.document_id,
            "processing_time": processing_time,
            "content_length": len(content),
            "processing_speed": len(content) / processing_time if processing_time > 0 else 0
        }

        print(f"\nğŸ [ç»“æŸæ—¥å¿—] è®°å½•å¤„ç†å®Œæˆ:")
        self._print_json_log(end_log, "âœ…")

        # è®°å½•åˆ°æ—¥å¿—å†å²
        self.log_history.append(end_log)

        # ğŸ¯ å¢å¼ºè¿”å›ç»“æœ - æ·»åŠ æ—¥å¿—ä¿¡æ¯
        result["logging_info"] = {
            "start_time": start_time,
            "end_time": end_time,
            "processing_time": processing_time,
            "log_entries": len(self.log_history),
            "decorator_name": self.__class__.__name__
        }

        print(f"ğŸ“Š [å¤„ç†ç»Ÿè®¡] æ€»è€—æ—¶: {processing_time:.3f} ç§’")
        print(f"âš¡ [å¤„ç†é€Ÿåº¦] {len(content) / processing_time:.1f} å­—ç¬¦/ç§’")
        print(f"ğŸ“ [æ—¥å¿—è£…é¥°å™¨] ================ æ‰§è¡Œå®Œæˆ ================\n")

        return result

    def _print_json_log(self, log_entry: Dict[str, Any], icon: str = "ğŸ“‹"):
        """
        æ‰“å°JSONæ ¼å¼çš„æ—¥å¿—

        Args:
            log_entry: æ—¥å¿—æ¡ç›®
            icon: æ—¥å¿—å›¾æ ‡
        """
        try:
            formatted_log = json.dumps(log_entry, indent=2, ensure_ascii=False)
            print(f"{icon} [JSONæ—¥å¿—]")
            for line in formatted_log.split('\n'):
                print(f"   {line}")
        except Exception as e:
            print(f"âŒ [æ—¥å¿—é”™è¯¯] æ— æ³•æ ¼å¼åŒ–æ—¥å¿—: {str(e)}")
            print(f"ğŸ“„ [åŸå§‹æ—¥å¿—] {log_entry}")

    def get_log_history(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—å†å²"""
        return self.log_history.copy()

    def get_log_summary(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—æ‘˜è¦"""
        if not self.log_history:
            return {"total_logs": 0, "message": "æš‚æ— æ—¥å¿—è®°å½•"}

        total_processing_time = sum(
            log.get("processing_time", 0)
            for log in self.log_history
            if log.get("event") == "processing_completed"
        )

        return {
            "total_logs": len(self.log_history),
            "completed_processes": len([log for log in self.log_history if log.get("event") == "processing_completed"]),
            "total_processing_time": total_processing_time,
            "avg_processing_time": total_processing_time / len([log for log in self.log_history if log.get("event") == "processing_completed"]) if [log for log in self.log_history if log.get("event") == "processing_completed"] else 0,
            "decorator_type": self.__class__.__name__
        }

    def batch_process(self, contents: List[str], context: ProcessingContext) -> List[Dict[str, Any]]:
        logger.log(self.log_level, f"å¼€å§‹æ‰¹é‡å¤„ç† {len(contents)} ä¸ªæ–‡æ¡£")
        start_time = time.time()
        results = super().batch_process(contents, context)
        end_time = time.time()

        logger.log(self.log_level, f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        return results


class CacheDecorator(ProcessorDecorator):
    """
    ç¼“å­˜è£…é¥°å™¨ - è£…é¥°å™¨æ¨¡å¼çš„å…·ä½“è£…é¥°å™¨å®ç°

    è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼çš„"å…·ä½“è£…é¥°å™¨"ä¹‹äºŒï¼Œä¸ºå¤„ç†å™¨æ·»åŠ ç¼“å­˜åŠŸèƒ½ã€‚
    å±•ç¤ºäº†è£…é¥°å™¨å¦‚ä½•æä¾›æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½è€Œä¸å½±å“åŸå§‹å¤„ç†é€»è¾‘ã€‚

    å¢å¼ºåŠŸèƒ½ï¼š
    - æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆ
    - LRUç¼“å­˜ç­–ç•¥
    - ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
    - JSONæ ¼å¼çš„ç¼“å­˜åˆ†æ
    """

    def __init__(self, processor: DocumentProcessor, cache_size: int = 100):
        """
        åˆå§‹åŒ–ç¼“å­˜è£…é¥°å™¨

        Args:
            processor: è¢«è£…é¥°çš„å¤„ç†å™¨
            cache_size: ç¼“å­˜å®¹é‡
        """
        super().__init__(processor)
        self.cache_size = cache_size
        self.cache = {}  # ç¼“å­˜å­˜å‚¨
        self.cache_order = []  # LRUé¡ºåº
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0
        }

        print(f"ğŸ’¾ [ç¼“å­˜è£…é¥°å™¨] åˆå§‹åŒ–ç¼“å­˜åŠŸèƒ½")
        print(f"ğŸ“Š [ç¼“å­˜é…ç½®] å®¹é‡: {cache_size} é¡¹")
        print(f"ğŸ”‘ [ç¼“å­˜ç­–ç•¥] LRU (æœ€è¿‘æœ€å°‘ä½¿ç”¨)")
        print(f"ğŸ“ˆ [ç»Ÿè®¡åˆå§‹åŒ–] å‘½ä¸­ç‡ã€æœªå‘½ä¸­ç‡ã€é©±é€ç»Ÿè®¡")

    def _get_cache_key(self, content: str, context: ProcessingContext) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”® - æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆç®—æ³•

        ä½¿ç”¨å†…å®¹çš„MD5å“ˆå¸Œå’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®ï¼Œ
        ç¡®ä¿ç›¸åŒå†…å®¹å’Œä¸Šä¸‹æ–‡èƒ½å¤Ÿå‘½ä¸­ç¼“å­˜ã€‚

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            str: å”¯ä¸€çš„ç¼“å­˜é”®
        """
        print(f"ğŸ”‘ [ç¼“å­˜é”®ç”Ÿæˆ] å¼€å§‹ç”Ÿæˆç¼“å­˜é”®...")

        # å†…å®¹å“ˆå¸Œ
        content_hash = hashlib.md5(content.encode()).hexdigest()
        print(f"ğŸ“ [å†…å®¹å“ˆå¸Œ] é•¿åº¦: {len(content)} -> MD5: {content_hash[:8]}...")

        # ä¸Šä¸‹æ–‡å“ˆå¸Œ
        context_data = {
            "user_id": context.user_id,
            "metadata": context.metadata
        }
        context_str = json.dumps(context_data, sort_keys=True)
        context_hash = hashlib.md5(context_str.encode()).hexdigest()
        print(f"ğŸ·ï¸  [ä¸Šä¸‹æ–‡å“ˆå¸Œ] ç”¨æˆ·: {context.user_id} -> MD5: {context_hash[:8]}...")

        cache_key = f"{content_hash}_{context_hash}"
        print(f"ğŸ”‘ [æœ€ç»ˆç¼“å­˜é”®] {cache_key[:16]}...")
        return cache_key

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        å¸¦ç¼“å­˜çš„æ–‡æ¡£å¤„ç†æ–¹æ³•

        è£…é¥°å™¨æ¨¡å¼çš„ç¼“å­˜å®ç°ï¼š
        1. å‰ç½®å¤„ç†ï¼šæ£€æŸ¥ç¼“å­˜
        2. æ¡ä»¶è°ƒç”¨ï¼šç¼“å­˜æœªå‘½ä¸­æ—¶æ‰è°ƒç”¨è¢«è£…é¥°å¯¹è±¡
        3. åç½®å¤„ç†ï¼šå°†ç»“æœåŠ å…¥ç¼“å­˜

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: ç¼“å­˜æˆ–æ–°è®¡ç®—çš„å¤„ç†ç»“æœ
        """
        print(f"\nğŸ’¾ [ç¼“å­˜è£…é¥°å™¨] ================ å¼€å§‹æ‰§è¡Œ ================")
        self._print_decorator_info("ç¼“å­˜æ£€æŸ¥é˜¶æ®µ")

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(content, context)

        # ğŸ”¥ å‰ç½®å¢å¼ºåŠŸèƒ½ - ç¼“å­˜æŸ¥æ‰¾
        print(f"ğŸ” [ç¼“å­˜æŸ¥æ‰¾] æ£€æŸ¥ç¼“å­˜é”®: {cache_key[:16]}...")

        if cache_key in self.cache:
            # ç¼“å­˜å‘½ä¸­
            self.cache_stats["hits"] += 1
            print(f"ğŸ¯ [ç¼“å­˜å‘½ä¸­] ç¼“å­˜é”® {cache_key[:16]}... æ‰¾åˆ°ç¼“å­˜!")

            cached_result = self.cache[cache_key].copy()
            cached_result["cached"] = True
            cached_result["cache_timestamp"] = time.time()
            cached_result["cache_key"] = cache_key[:16] + "..."

            # è·å–ç¼“å­˜å¹´é¾„
            cache_age = time.time() - cached_result.get("original_timestamp", time.time())
            cached_result["cache_age"] = cache_age

            print(f"ğŸ“… [ç¼“å­˜ä¿¡æ¯] ç¼“å­˜å¹´é¾„: {cache_age:.2f} ç§’")

            # æ‰“å°ç¼“å­˜å‘½ä¸­ä¿¡æ¯
            cache_hit_info = {
                "cache_key": cache_key,
                "cache_age_seconds": cache_age,
                "document_id": context.document_id,
                "hit_count": self.cache_stats["hits"]
            }
            self._print_cache_info(cache_hit_info, "ğŸ¯")

            print(f"ğŸ’¾ [ç¼“å­˜è£…é¥°å™¨] ================ ç¼“å­˜å‘½ä¸­å®Œæˆ ================")
            return cached_result
        else:
            # ç¼“å­˜æœªå‘½ä¸­
            self.cache_stats["misses"] += 1
            print(f"âŒ [ç¼“å­˜æœªå‘½ä¸­] ç¼“å­˜é”® {cache_key[:16]}... æœªæ‰¾åˆ°ç¼“å­˜")
            print(f"ğŸ“Š [ç¼“å­˜ç»Ÿè®¡] å‘½ä¸­: {self.cache_stats['hits']}, æœªå‘½ä¸­: {self.cache_stats['misses']}")

            # ğŸ¯ è°ƒç”¨è¢«è£…é¥°å¯¹è±¡ - ç¼“å­˜æœªå‘½ä¸­æ—¶æ‰æ‰§è¡Œ
            print(f"ğŸ”„ [å§”æ‰˜è°ƒç”¨] ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨è¢«è£…é¥°å¯¹è±¡...")
            result = super().process(content, context)
            print(f"ğŸ”„ [å§”æ‰˜è¿”å›] è¢«è£…é¥°å¯¹è±¡å¤„ç†å®Œæˆ")

            # ğŸ”¥ åç½®å¢å¼ºåŠŸèƒ½ - æ·»åŠ åˆ°ç¼“å­˜
            print(f"ğŸ’¾ [ç¼“å­˜å­˜å‚¨] å°†ç»“æœæ·»åŠ åˆ°ç¼“å­˜...")
            result["cached"] = False
            result["cache_key"] = cache_key[:16] + "..."
            result["original_timestamp"] = time.time()

            # æ·»åŠ åˆ°ç¼“å­˜
            self._add_to_cache(cache_key, result)

            print(f"ğŸ’¾ [ç¼“å­˜è£…é¥°å™¨] ================ ç¼“å­˜å­˜å‚¨å®Œæˆ ================")
            return result

    def _add_to_cache(self, key: str, result: Dict[str, Any]):
        """
        æ·»åŠ ç»“æœåˆ°ç¼“å­˜ - LRUç­–ç•¥å®ç°

        Args:
            key: ç¼“å­˜é”®
            result: å¤„ç†ç»“æœ
        """
        print(f"ğŸ“¦ [ç¼“å­˜ç®¡ç†] æ·»åŠ æ–°é¡¹åˆ°ç¼“å­˜...")

        # æ£€æŸ¥ç¼“å­˜å®¹é‡
        if len(self.cache) >= self.cache_size:
            # ç¼“å­˜æ»¡ï¼Œæ‰§è¡ŒLRUé©±é€
            print(f"âš ï¸  [ç¼“å­˜æ»¡] ç¼“å­˜å·²æ»¡ ({len(self.cache)}/{self.cache_size})ï¼Œæ‰§è¡ŒLRUé©±é€")
            oldest_key = self.cache_order.pop(0)
            del self.cache[oldest_key]
            self.cache_stats["evictions"] += 1
            print(f"ğŸ—‘ï¸  [LRUé©±é€] é©±é€æœ€æ—§ç¼“å­˜é¡¹: {oldest_key[:16]}...")

        # æ·»åŠ æ–°ç¼“å­˜é¡¹
        self.cache[key] = result.copy()
        self.cache_order.append(key)

        # æ›´æ–°ç¼“å­˜é¡ºåºï¼ˆLRUï¼‰
        if key in self.cache_order:
            self.cache_order.remove(key)
        self.cache_order.append(key)

        print(f"âœ… [ç¼“å­˜æ·»åŠ ] æˆåŠŸæ·»åŠ ç¼“å­˜é¡¹: {key[:16]}...")
        print(f"ğŸ“Š [ç¼“å­˜çŠ¶æ€] å½“å‰ä½¿ç”¨: {len(self.cache)}/{self.cache_size}")

    def _print_cache_info(self, cache_info: Dict[str, Any], icon: str = "ğŸ’¾"):
        """
        æ‰“å°JSONæ ¼å¼çš„ç¼“å­˜ä¿¡æ¯

        Args:
            cache_info: ç¼“å­˜ä¿¡æ¯
            icon: ä¿¡æ¯å›¾æ ‡
        """
        try:
            formatted_info = json.dumps(cache_info, indent=2, ensure_ascii=False)
            print(f"{icon} [ç¼“å­˜ä¿¡æ¯]")
            for line in formatted_info.split('\n'):
                print(f"   {line}")
        except Exception as e:
            print(f"âŒ [ç¼“å­˜é”™è¯¯] æ— æ³•æ ¼å¼åŒ–ç¼“å­˜ä¿¡æ¯: {str(e)}")
            print(f"ğŸ“„ [åŸå§‹ä¿¡æ¯] {cache_info}")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        hit_rate = self.cache_stats["hits"] / (self.cache_stats["hits"] + self.cache_stats["misses"]) if (self.cache_stats["hits"] + self.cache_stats["misses"]) > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_size": self.cache_size,
            "cache_usage": len(self.cache) / self.cache_size,
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "evictions": self.cache_stats["evictions"],
            "hit_rate": hit_rate,
            "total_requests": self.cache_stats["hits"] + self.cache_stats["misses"],
            "decorator_type": self.__class__.__name__
        }

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.cache_order.clear()
        print(f"ğŸ—‘ï¸  [ç¼“å­˜æ¸…ç©º] ç¼“å­˜å·²æ¸…ç©º")

    

class PerformanceMonitorDecorator(ProcessorDecorator):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨ - ç›‘æ§å¤„ç†æ€§èƒ½"""

    def __init__(self, processor: DocumentProcessor):
        super().__init__(processor)
        self.performance_stats = {
            "total_requests": 0,
            "total_time": 0.0,
            "avg_time": 0.0,
            "max_time": 0.0,
            "min_time": float('inf')
        }

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        start_time = time.time()
        result = super().process(content, context)
        end_time = time.time()

        processing_time = end_time - start_time

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        self._update_stats(processing_time)

        # æ·»åŠ æ€§èƒ½ä¿¡æ¯åˆ°ç»“æœ
        result["performance"] = {
            "processing_time": processing_time,
            "content_length": len(content),
            "chars_per_second": len(content) / processing_time if processing_time > 0 else 0
        }

        print(f"â±ï¸  æ€§èƒ½ç»Ÿè®¡: {processing_time:.3f}s, {len(content)}/s")
        return result

    def _update_stats(self, processing_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.performance_stats["total_requests"] += 1
        self.performance_stats["total_time"] += processing_time
        self.performance_stats["avg_time"] = (
            self.performance_stats["total_time"] /
            self.performance_stats["total_requests"]
        )
        self.performance_stats["max_time"] = max(
            self.performance_stats["max_time"], processing_time
        )
        self.performance_stats["min_time"] = min(
            self.performance_stats["min_time"], processing_time
        )

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        stats = self.performance_stats.copy()
        stats["min_time"] = stats["min_time"] if stats["min_time"] != float('inf') else 0
        return stats


class RetryDecorator(ProcessorDecorator):
    """é‡è¯•è£…é¥°å™¨ - å¤„ç†å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•"""

    def __init__(self, processor: DocumentProcessor, max_retries: int = 3, delay: float = 1.0):
        super().__init__(processor)
        self.max_retries = max_retries
        self.delay = delay

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ ç¬¬ {attempt} æ¬¡é‡è¯•å¤„ç†: {context.document_id}")
                    time.sleep(self.delay * attempt)  # æŒ‡æ•°é€€é¿

                result = super().process(content, context)
                result["retry_attempts"] = attempt

                if attempt > 0:
                    print(f"âœ… é‡è¯•æˆåŠŸ: {context.document_id}")

                return result

            except Exception as e:
                last_exception = e
                print(f"âŒ å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        print(f"ğŸ’¥ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥: {context.document_id}")
        raise Exception(f"å¤„ç†å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {str(last_exception)}")


class AIEnhancementDecorator(ProcessorDecorator):
    """
    AIå¢å¼ºè£…é¥°å™¨ - è£…é¥°å™¨æ¨¡å¼çš„æ ¸å¿ƒè£…é¥°å™¨å®ç°

    è¿™æ˜¯è£…é¥°å™¨æ¨¡å¼æœ€é‡è¦çš„"å…·ä½“è£…é¥°å™¨"ï¼Œä¸ºå¤„ç†å™¨æ·»åŠ AIæ™ºèƒ½åˆ†æåŠŸèƒ½ã€‚
    å±•ç¤ºäº†è£…é¥°å™¨å¦‚ä½•é›†æˆå¤æ‚çš„ç¬¬ä¸‰æ–¹æœåŠ¡ï¼ˆAI APIï¼‰è€Œä¸å½±å“åŸå§‹ä»£ç ç»“æ„ã€‚

    å¢å¼ºåŠŸèƒ½ï¼š
    - æƒ…æ„Ÿåˆ†æ
    - æ–‡æœ¬æ‘˜è¦
    - å…³é”®è¯æå–
    - æ™ºèƒ½åˆ†ç±»
    - JSONæ ¼å¼çš„AIåˆ†æç»“æœ
    - AI APIè°ƒç”¨çš„è¯¦ç»†æ—¥å¿—
    """

    def __init__(self, processor: DocumentProcessor, ai_model: str = "deepseek"):
        """
        åˆå§‹åŒ–AIå¢å¼ºè£…é¥°å™¨

        Args:
            processor: è¢«è£…é¥°çš„å¤„ç†å™¨
            ai_model: ä½¿ç”¨çš„AIæ¨¡å‹
        """
        super().__init__(processor)
        self.ai_model = ai_model
        self.ai_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "total_api_calls": 0
        }

        print(f"ğŸ¤– [AIè£…é¥°å™¨] åˆå§‹åŒ–AIå¢å¼ºåŠŸèƒ½")
        print(f"ğŸ§  [AIæ¨¡å‹] ä½¿ç”¨æ¨¡å‹: {ai_model}")
        print(f"ğŸ“Š [èƒ½åŠ›é…ç½®] æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æ‘˜è¦ã€å…³é”®è¯æå–")
        print(f"ğŸ“ˆ [ç»Ÿè®¡åˆå§‹åŒ–] APIè°ƒç”¨ã€æˆåŠŸç‡ç»Ÿè®¡")

    def process(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        å¸¦AIå¢å¼ºçš„æ–‡æ¡£å¤„ç†æ–¹æ³•

        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†è£…é¥°å™¨æ¨¡å¼å¤„ç†å¤æ‚å¢å¼ºåŠŸèƒ½çš„èƒ½åŠ›ï¼š
        1. å…ˆè°ƒç”¨è¢«è£…é¥°å¯¹è±¡è¿›è¡ŒåŸºç¡€å¤„ç†
        2. ç„¶åæ·»åŠ AIåˆ†æå¢å¼ºåŠŸèƒ½
        3. æœ€ååˆå¹¶æ‰€æœ‰ç»“æœè¿”å›

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: åŒ…å«AIåˆ†æå¢å¼ºçš„å¤„ç†ç»“æœ
        """
        print(f"\nğŸ¤– [AIè£…é¥°å™¨] ================ å¼€å§‹æ‰§è¡Œ ================")
        self._print_decorator_info("AIå¢å¼ºé˜¶æ®µ")

        # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šè°ƒç”¨è¢«è£…é¥°å¯¹è±¡è¿›è¡ŒåŸºç¡€å¤„ç†
        print(f"ğŸ”„ [æ­¥éª¤1] è°ƒç”¨è¢«è£…é¥°å¯¹è±¡è¿›è¡ŒåŸºç¡€å¤„ç†...")
        base_start_time = time.time()
        base_result = super().process(content, context)
        base_end_time = time.time()
        base_processing_time = base_end_time - base_start_time

        print(f"âœ… [åŸºç¡€å¤„ç†å®Œæˆ] è€—æ—¶: {base_processing_time:.3f} ç§’")

        # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šAIå¢å¼ºåŠŸèƒ½
        print(f"\nğŸ§  [æ­¥éª¤2] å¼€å§‹AIæ™ºèƒ½å¢å¼ºåˆ†æ...")
        ai_start_time = time.time()

        # å¢åŠ åˆ†æç»Ÿè®¡
        self.ai_stats["total_analyses"] += 1

        try:
            # è°ƒç”¨çœŸå®çš„AIåˆ†æ
            ai_analysis = self._perform_ai_analysis(content, context)
            ai_end_time = time.time()
            ai_processing_time = ai_end_time - ai_start_time

            # æ›´æ–°æˆåŠŸç»Ÿè®¡
            self.ai_stats["successful_analyses"] += 1

            print(f"âœ… [AIåˆ†æå®Œæˆ] è€—æ—¶: {ai_processing_time:.3f} ç§’")

            # ğŸ”¥ ç¬¬ä¸‰æ­¥ï¼šå¢å¼ºè¿”å›ç»“æœ
            base_result["ai_enhancement"] = {
                "model": self.ai_model,
                "provider": "deepseek",
                "analysis": ai_analysis,
                "processed_at": time.time(),
                "ai_processing_time": ai_processing_time,
                "total_processing_time": base_processing_time + ai_processing_time,
                "enhancement_successful": True
            }

            print(f"ğŸ“Š [æ€§èƒ½å¯¹æ¯”] åŸºç¡€å¤„ç†: {base_processing_time:.3f}s, AIåˆ†æ: {ai_processing_time:.3f}s")
            print(f"ğŸ“ˆ [æ€»ä½“æ€§èƒ½] æ€»è€—æ—¶: {base_processing_time + ai_processing_time:.3f}s")

        except Exception as e:
            # AIåˆ†æå¤±è´¥æ—¶çš„é™çº§å¤„ç†
            ai_end_time = time.time()
            ai_processing_time = ai_end_time - ai_start_time

            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self.ai_stats["failed_analyses"] += 1

            print(f"âŒ [AIåˆ†æå¤±è´¥] {str(e)}")
            print(f"ğŸ”„ [é™çº§å¤„ç†] ä½¿ç”¨åŸºç¡€åˆ†æç»“æœ")

            # é™çº§åˆ†æ
            fallback_analysis = self._create_fallback_analysis(content)

            base_result["ai_enhancement"] = {
                "model": self.ai_model,
                "provider": "fallback",
                "analysis": fallback_analysis,
                "processed_at": time.time(),
                "ai_processing_time": ai_processing_time,
                "error": str(e),
                "enhancement_successful": False,
                "fallback_mode": True
            }

        print(f"ğŸ¤– [AIè£…é¥°å™¨] ================ æ‰§è¡Œå®Œæˆ ================")
        return base_result

    def _perform_ai_analysis(self, content: str, context: ProcessingContext) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„AIåˆ†æ - è°ƒç”¨å¤šä¸ªAI API

        Args:
            content: æ–‡æ¡£å†…å®¹
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: å®Œæ•´çš„AIåˆ†æç»“æœ
        """
        print(f"ğŸ§  [AIåˆ†æ] å¼€å§‹è°ƒç”¨DeepSeek APIè¿›è¡Œå¤šç»´åº¦åˆ†æ...")

        try:
            # è·å–AIæœåŠ¡å®ä¾‹
            ai_service = get_ai_service("deepseek")
            print(f"ğŸ”— [APIè¿æ¥] æˆåŠŸè¿æ¥åˆ°DeepSeekæœåŠ¡")

            # ğŸ¯ å¹¶è¡Œæ‰§è¡Œå¤šç§AIåˆ†æ
            print(f"\nğŸ“Š [å¹¶è¡Œåˆ†æ] å¼€å§‹æ‰§è¡Œ3ç§AIåˆ†æ...")

            # 1. æƒ…æ„Ÿåˆ†æ
            print(f"ğŸ’­ [åˆ†æ1] æƒ…æ„Ÿåˆ†æ...")
            sentiment_start = time.time()
            sentiment_result = ai_service.sentiment_analysis(content)
            sentiment_time = time.time() - sentiment_start
            self.ai_stats["total_api_calls"] += 1
            print(f"âœ… [æƒ…æ„Ÿåˆ†æå®Œæˆ] è€—æ—¶: {sentiment_time:.3f}s")

            # 2. æ–‡æœ¬æ‘˜è¦
            print(f"ğŸ“ [åˆ†æ2] æ–‡æœ¬æ‘˜è¦...")
            summary_start = time.time()
            summary_result = ai_service.extract_summary(content, 200)
            summary_time = time.time() - summary_start
            self.ai_stats["total_api_calls"] += 1
            print(f"âœ… [æ–‡æœ¬æ‘˜è¦å®Œæˆ] è€—æ—¶: {summary_time:.3f}s")

            # 3. å…³é”®è¯æå–
            print(f"ğŸ” [åˆ†æ3] å…³é”®è¯æå–...")
            keywords_start = time.time()
            keywords_result = ai_service.extract_keywords(content, 10)
            keywords_time = time.time() - keywords_start
            self.ai_stats["total_api_calls"] += 1
            print(f"âœ… [å…³é”®è¯æå–å®Œæˆ] è€—æ—¶: {keywords_time:.3f}s")

            # ğŸ”¥ è§£æå’Œç»„åˆAIåˆ†æç»“æœ
            print(f"\nğŸ”§ [ç»“æœå¤„ç†] è§£æå’Œç»„åˆAIåˆ†æç»“æœ...")
            analysis = self._parse_ai_results(
                sentiment_result, summary_result, keywords_result, content
            )

            # æ·»åŠ APIè°ƒç”¨ç»Ÿè®¡
            analysis["api_call_stats"] = {
                "total_calls": 3,
                "sentiment_time": sentiment_time,
                "summary_time": summary_time,
                "keywords_time": keywords_time,
                "total_ai_time": sentiment_time + summary_time + keywords_time
            }

            print(f"ğŸ“Š [APIç»Ÿè®¡] æ€»è°ƒç”¨æ¬¡æ•°: {self.ai_stats['total_api_calls']}")
            print(f"ğŸ¯ [AIåˆ†æ] å¤šç»´åº¦åˆ†æå®Œæˆ!")

            return analysis

        except Exception as e:
            print(f"ğŸ’¥ [AIåˆ†æé”™è¯¯] AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"AIåˆ†æå¤±è´¥: {str(e)}")

    def _parse_ai_results(self, sentiment_result: Dict, summary_result: Dict,
                         keywords_result: Dict, content: str) -> Dict[str, Any]:
        """
        è§£æAIåˆ†æç»“æœå¹¶æå–JSONæ•°æ®

        Args:
            sentiment_result: æƒ…æ„Ÿåˆ†æç»“æœ
            summary_result: æ‘˜è¦ç»“æœ
            keywords_result: å…³é”®è¯ç»“æœ
            content: åŸå§‹å†…å®¹

        Returns:
            Dict[str, Any]: è§£æåçš„å®Œæ•´åˆ†æç»“æœ
        """
        print(f"ğŸ” [ç»“æœè§£æ] å¼€å§‹è§£æAIè¿”å›çš„JSONæ•°æ®...")

        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        word_count = len(content.split())
        sentences = content.count('.') + content.count('!') + content.count('?')

        analysis = {
            "basic_stats": {
                "word_count": word_count,
                "sentence_count": sentences,
                "character_count": len(content),
                "avg_sentence_length": word_count / sentences if sentences > 0 else 0
            }
        }

        # ğŸ¯ è§£ææƒ…æ„Ÿåˆ†æç»“æœ
        print(f"ğŸ’­ [æƒ…æ„Ÿåˆ†æ] è§£ææƒ…æ„Ÿåˆ†æJSON...")
        if sentiment_result.get("success"):
            try:
                if isinstance(sentiment_result["content"], str):
                    sentiment_data = json.loads(sentiment_result["content"])
                    print(f"âœ… [æƒ…æ„ŸJSON] æˆåŠŸè§£ææƒ…æ„Ÿåˆ†æJSON")

                    analysis["sentiment"] = {
                        "emotion": sentiment_data.get("sentiment", "neutral"),
                        "confidence": sentiment_data.get("confidence", 0.5),
                        "key_emotions": sentiment_data.get("key_emotions", []),
                        "emotional_intensity": sentiment_data.get("emotional_intensity", "medium")
                    }

                    # æ‰“å°æƒ…æ„Ÿåˆ†æJSON
                    self._print_ai_json_result(sentiment_data, "ğŸ’­", "æƒ…æ„Ÿåˆ†æ")
                else:
                    analysis["sentiment"] = sentiment_result["content"]
            except json.JSONDecodeError as e:
                print(f"âš ï¸  [æƒ…æ„ŸJSON] JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§å¤„ç†: {str(e)}")
                analysis["sentiment"] = {
                    "emotion": "neutral",
                    "confidence": 0.5,
                    "parse_error": str(e)
                }
        else:
            print(f"âŒ [æƒ…æ„ŸAPI] æƒ…æ„Ÿåˆ†æAPIè°ƒç”¨å¤±è´¥")
            analysis["sentiment"] = {
                "emotion": "neutral",
                "confidence": 0.0,
                "error": sentiment_result.get("error", "æœªçŸ¥é”™è¯¯")
            }

        # ğŸ¯ è§£ææ‘˜è¦ç»“æœ
        print(f"ğŸ“ [æ–‡æœ¬æ‘˜è¦] å¤„ç†æ‘˜è¦ç»“æœ...")
        if summary_result.get("success"):
            analysis["summary"] = {
                "text": summary_result["content"],
                "type": "extractive",
                "length": len(summary_result["content"])
            }
            print(f"âœ… [æ‘˜è¦å®Œæˆ] æ‘˜è¦é•¿åº¦: {len(summary_result['content'])} å­—ç¬¦")
        else:
            analysis["summary"] = {
                "text": content[:200] + "..." if len(content) > 200 else content,
                "type": "fallback",
                "error": summary_result.get("error", "æœªçŸ¥é”™è¯¯")
            }

        # ğŸ¯ è§£æå…³é”®è¯ç»“æœ
        print(f"ğŸ” [å…³é”®è¯æå–] è§£æå…³é”®è¯JSON...")
        if keywords_result.get("success"):
            try:
                if isinstance(keywords_result["content"], str):
                    keywords_data = json.loads(keywords_result["content"])
                    print(f"âœ… [å…³é”®è¯JSON] æˆåŠŸè§£æå…³é”®è¯JSON")

                    analysis["keywords"] = {
                        "list": keywords_data.get("keywords", [])[:10],
                        "categories": keywords_data.get("categories", []),
                        "key_phrases": keywords_data.get("key_phrases", [])
                    }

                    # æ‰“å°å…³é”®è¯åˆ†æJSON
                    self._print_ai_json_result(keywords_data, "ğŸ”", "å…³é”®è¯åˆ†æ")
                else:
                    analysis["keywords"] = keywords_result["content"]
            except json.JSONDecodeError as e:
                print(f"âš ï¸  [å…³é”®è¯JSON] JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§å¤„ç†: {str(e)}")
                # é™çº§å¤„ç†ï¼šç®€å•æå–è¯è¯­
                words = content.split()
                analysis["keywords"] = {
                    "list": list(set([word for word in words if len(word) > 3]))[:10],
                    "categories": [],
                    "parse_error": str(e)
                }
        else:
            print(f"âŒ [å…³é”®è¯API] å…³é”®è¯æå–APIè°ƒç”¨å¤±è´¥")
            analysis["keywords"] = {
                "list": ["æ–‡æ¡£", "å†…å®¹", "åˆ†æ"],
                "categories": [],
                "error": keywords_result.get("error", "æœªçŸ¥é”™è¯¯")
            }

        # æ–‡æ¡£å¤æ‚åº¦åˆ†æ
        analysis["complexity"] = (
            "high" if word_count > 800 else
            "medium" if word_count > 300 else
            "low"
        )

        print(f"ğŸ“Š [å¤æ‚åº¦è¯„ä¼°] æ–‡æ¡£å¤æ‚åº¦: {analysis['complexity']}")
        return analysis

    def _print_ai_json_result(self, json_data: Dict[str, Any], icon: str, analysis_type: str):
        """
        æ‰“å°AIè¿”å›çš„JSONåˆ†æç»“æœ

        Args:
            json_data: AIè¿”å›çš„JSONæ•°æ®
            icon: å›¾æ ‡
            analysis_type: åˆ†æç±»å‹
        """
        print(f"\n{icon} [AI JSONç»“æœ] {analysis_type}:")
        print("=" * 40)
        try:
            formatted_json = json.dumps(json_data, indent=2, ensure_ascii=False)
            for line in formatted_json.split('\n'):
                print(f"   {line}")
        except Exception as e:
            print(f"âŒ [JSONæ ¼å¼åŒ–é”™è¯¯] {str(e)}")
            print(f"ğŸ“„ [åŸå§‹æ•°æ®] {json_data}")
        print("=" * 40)

    def _create_fallback_analysis(self, content: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé™çº§åˆ†æç»“æœ - AIæœåŠ¡ä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆ

        Args:
            content: æ–‡æ¡£å†…å®¹

        Returns:
            Dict[str, Any]: é™çº§åˆ†æç»“æœ
        """
        print(f"ğŸ”„ [é™çº§åˆ†æ] åˆ›å»ºåŸºç¡€åˆ†æç»“æœ...")

        word_count = len(content.split())
        sentences = content.count('.') + content.count('!') + content.count('?')

        return {
            "basic_stats": {
                "word_count": word_count,
                "sentence_count": sentences,
                "character_count": len(content),
                "avg_sentence_length": word_count / sentences if sentences > 0 else 0
            },
            "sentiment": {
                "emotion": "neutral",
                "confidence": 0.5,
                "fallback_mode": True
            },
            "summary": {
                "text": content[:200] + "..." if len(content) > 200 else content,
                "type": "fallback",
                "fallback_mode": True
            },
            "keywords": {
                "list": ["æ–‡æ¡£", "å¤„ç†", "åˆ†æ"],
                "categories": ["é€šç”¨"],
                "fallback_mode": True
            },
            "complexity": "high" if word_count > 500 else "medium" if word_count > 200 else "low",
            "fallback_mode": True,
            "error_reason": "AIæœåŠ¡ä¸å¯ç”¨"
        }

    def get_ai_stats(self) -> Dict[str, Any]:
        """è·å–AIåˆ†æç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (
            self.ai_stats["successful_analyses"] / self.ai_stats["total_analyses"]
            if self.ai_stats["total_analyses"] > 0 else 0
        )

        return {
            "total_analyses": self.ai_stats["total_analyses"],
            "successful_analyses": self.ai_stats["successful_analyses"],
            "failed_analyses": self.ai_stats["failed_analyses"],
            "success_rate": success_rate,
            "total_api_calls": self.ai_stats["total_api_calls"],
            "ai_model": self.ai_model,
            "decorator_type": self.__class__.__name__
        }

    def _simulate_ai_analysis(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek APIè¿›è¡ŒçœŸå®çš„AIæ–‡æ¡£åˆ†æ"""
        try:
            ai_service = get_ai_service("deepseek")

            # å¹¶è¡Œè¿›è¡Œå¤šç§åˆ†æ
            sentiment_result = ai_service.sentiment_analysis(content)
            summary_result = ai_service.extract_summary(content, 200)
            keywords_result = ai_service.extract_keywords(content, 10)

            # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            word_count = len(content.split())
            sentences = content.count('.') + content.count('!') + content.count('?')

            # ç»„åˆåˆ†æç»“æœ
            analysis = {
                "stats": {
                    "word_count": word_count,
                    "sentence_count": sentences,
                    "avg_sentence_length": word_count / sentences if sentences > 0 else 0
                }
            }

            # æƒ…æ„Ÿåˆ†æç»“æœ
            if sentiment_result["success"]:
                try:
                    import json
                    sentiment_data = json.loads(sentiment_result["content"])
                    analysis["sentiment"] = sentiment_data.get("sentiment", "neutral")
                    analysis["sentiment_confidence"] = sentiment_data.get("confidence", 0.5)
                    analysis["key_emotions"] = sentiment_data.get("key_emotions", [])
                except:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å¤„ç†
                    analysis["sentiment"] = "neutral"
                    analysis["sentiment_confidence"] = 0.5
            else:
                analysis["sentiment"] = "neutral"
                analysis["sentiment_confidence"] = 0.5
                analysis["sentiment_error"] = sentiment_result.get("error", "æœªçŸ¥é”™è¯¯")

            # æ‘˜è¦ç»“æœ
            if summary_result["success"]:
                analysis["summary"] = summary_result["content"]
            else:
                analysis["summary"] = content[:100] + "..." if len(content) > 100 else content
                analysis["summary_error"] = summary_result.get("error", "æœªçŸ¥é”™è¯¯")

            # å…³é”®è¯ç»“æœ
            if keywords_result["success"]:
                try:
                    import json
                    keywords_data = json.loads(keywords_result["content"])
                    analysis["keywords"] = keywords_data.get("keywords", [])
                    analysis["categories"] = keywords_data.get("categories", [])
                except:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å…³é”®è¯
                    words = content.split()
                    analysis["keywords"] = list(set([word for word in words if len(word) > 1]))[:10]
                    analysis["categories"] = []
            else:
                analysis["keywords"] = ["æ–‡æ¡£", "å¤„ç†", "åˆ†æ"]
                analysis["categories"] = []
                analysis["keywords_error"] = keywords_result.get("error", "æœªçŸ¥é”™è¯¯")

            # æ–‡æ¡£å¤æ‚åº¦åˆ†æ
            analysis["complexity"] = (
                "high" if word_count > 500 else
                "medium" if word_count > 200 else
                "low"
            )

            return analysis

        except Exception as e:
            # å‡ºé”™æ—¶çš„é™çº§å¤„ç†
            word_count = len(content.split())
            sentences = content.count('.') + content.count('!') + content.count('?')

            return {
                "sentiment": "neutral",
                "sentiment_confidence": 0.5,
                "complexity": "high" if word_count > 500 else "medium" if word_count > 200 else "low",
                "keywords": ["æ–‡æ¡£", "å¤„ç†", "åˆ†æ"],
                "categories": [],
                "summary": content[:100] + "..." if len(content) > 100 else content,
                "stats": {
                    "word_count": word_count,
                    "sentence_count": sentences,
                    "avg_sentence_length": word_count / sentences if sentences > 0 else 0
                },
                "fallback_mode": True,
                "error": str(e)
            }


# å‡½æ•°å¼è£…é¥°å™¨ç¤ºä¾‹
def validate_content(func: Callable) -> Callable:
    """å†…å®¹éªŒè¯è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯content
        if args and isinstance(args[1], str):
            content = args[1]
            if not content.strip():
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©º")
            if len(content) > 100000:
                raise ValueError("æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œè¶…è¿‡100000å­—ç¬¦")

        return func(*args, **kwargs)
    return wrapper


def rate_limit(max_requests: int = 10, time_window: int = 60) -> Callable:
    """é€Ÿç‡é™åˆ¶è£…é¥°å™¨"""
    requests = []

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()

            # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
            requests[:] = [req_time for req_time in requests if now - req_time < time_window]

            if len(requests) >= max_requests:
                raise Exception(f"é€Ÿç‡é™åˆ¶: {time_window}ç§’å†…æœ€å¤š{max_requests}ä¸ªè¯·æ±‚")

            requests.append(now)
            return func(*args, **kwargs)
        return wrapper
    return decorator


# ============================================================================
# è£…é¥°å™¨æ¨¡å¼å®Œæ•´æ¼”ç¤º
# ============================================================================

def demo_decorator_pattern():
    """
    è£…é¥°å™¨æ¨¡å¼å®Œæ•´æ¼”ç¤º

    è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†è£…é¥°å™¨æ¨¡å¼çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š
    1. åˆ›å»ºåŸºç¡€å¤„ç†å™¨ï¼ˆComponentï¼‰
    2. é€å±‚æ·»åŠ è£…é¥°å™¨ï¼ˆConcrete Decoratorsï¼‰
    3. å±•ç¤ºè£…é¥°å™¨é“¾çš„æ‰§è¡Œè¿‡ç¨‹
    4. å±•ç¤ºAIåŠŸèƒ½çš„JSONè¿”å›ç»“æœ
    """
    print("ğŸ­ [è£…é¥°å™¨æ¨¡å¼æ¼”ç¤º] ================ è£…é¥°å™¨æ¨¡å¼å®Œæ•´æ¼”ç¤º ================")
    print("ğŸ“š [è®¾è®¡æ¨¡å¼] è£…é¥°å™¨æ¨¡å¼ (Decorator Pattern)")
    print("ğŸ’¡ [æ ¸å¿ƒç†å¿µ] åŠ¨æ€åœ°ä¸ºå¯¹è±¡æ·»åŠ æ–°åŠŸèƒ½ï¼Œæ— éœ€ä¿®æ”¹å…¶ä»£ç ç»“æ„")
    print("=" * 80)

    # æ­¥éª¤1: åˆ›å»ºåŸºç¡€å¤„ç†å™¨ï¼ˆè£…é¥°å™¨æ¨¡å¼çš„Componentï¼‰
    print(f"\nğŸ­ [æ­¥éª¤1] åˆ›å»ºåŸºç¡€æ–‡æ¡£å¤„ç†å™¨ (Component)")
    print("-" * 50)
    base_processor = DocumentProcessor()
    print("âœ… [åŸºç¡€å¤„ç†å™¨] DocumentProcessor åˆ›å»ºå®Œæˆ")

    # æ­¥éª¤2: é€å±‚æ·»åŠ è£…é¥°å™¨ - å±•ç¤ºè£…é¥°å™¨çš„åŠ¨æ€ç»„åˆèƒ½åŠ›
    print(f"\nğŸ­ [æ­¥éª¤2] é€å±‚æ·»åŠ è£…é¥°å™¨ (Concrete Decorators)")
    print("-" * 50)

    # ç¬¬ä¸€å±‚ï¼šAIå¢å¼ºè£…é¥°å™¨
    print(f"\nğŸ¤– [è£…é¥°å±‚1] æ·»åŠ AIå¢å¼ºè£…é¥°å™¨...")
    ai_enhanced_processor = AIEnhancementDecorator(base_processor)

    # ç¬¬äºŒå±‚ï¼šé‡è¯•è£…é¥°å™¨
    print(f"ğŸ”„ [è£…é¥°å±‚2] æ·»åŠ é‡è¯•è£…é¥°å™¨...")
    retry_processor = RetryDecorator(ai_enhanced_processor, max_retries=2, delay=0.5)

    # ç¬¬ä¸‰å±‚ï¼šæ€§èƒ½ç›‘æ§è£…é¥°å™¨
    print(f"â±ï¸  [è£…é¥°å±‚3] æ·»åŠ æ€§èƒ½ç›‘æ§è£…é¥°å™¨...")
    performance_processor = PerformanceMonitorDecorator(retry_processor)

    # ç¬¬å››å±‚ï¼šç¼“å­˜è£…é¥°å™¨
    print(f"ğŸ’¾ [è£…é¥°å±‚4] æ·»åŠ ç¼“å­˜è£…é¥°å™¨...")
    cache_processor = CacheDecorator(performance_processor, cache_size=5)

    # ç¬¬äº”å±‚ï¼šæ—¥å¿—è£…é¥°å™¨ï¼ˆæœ€å¤–å±‚ï¼‰
    print(f"ğŸ“ [è£…é¥°å±‚5] æ·»åŠ æ—¥å¿—è£…é¥°å™¨ï¼ˆæœ€å¤–å±‚ï¼‰...")
    enhanced_processor = LoggingDecorator(cache_processor)

    print("âœ… [è£…é¥°å™¨é“¾] 5å±‚è£…é¥°å™¨ç»„åˆå®Œæˆ!")

    # æ­¥éª¤3: æ˜¾ç¤ºè£…é¥°å™¨é“¾ç»“æ„
    print(f"\nğŸ”— [æ­¥éª¤3] è£…é¥°å™¨é“¾ç»“æ„åˆ†æ")
    print("-" * 50)
    print("ğŸ“‹ [æ‰§è¡Œé¡ºåº] è°ƒç”¨é¡ºåºï¼ˆä»å¤–åˆ°å†…ï¼‰ï¼š")
    print("   1ï¸âƒ£ LoggingDecorator (æ—¥å¿—è®°å½•)")
    print("   2ï¸âƒ£ CacheDecorator (ç¼“å­˜æ£€æŸ¥)")
    print("   3ï¸âƒ£ PerformanceMonitorDecorator (æ€§èƒ½ç›‘æ§)")
    print("   4ï¸âƒ£ RetryDecorator (é‡è¯•æœºåˆ¶)")
    print("   5ï¸âƒ£ AIEnhancementDecorator (AIåˆ†æ)")
    print("   6ï¸âƒ£ DocumentProcessor (åŸºç¡€å¤„ç†)")

    # æ­¥éª¤4: åˆ›å»ºæµ‹è¯•æ–‡æ¡£å’Œä¸Šä¸‹æ–‡
    print(f"\nğŸ“„ [æ­¥éª¤4] åˆ›å»ºæµ‹è¯•æ–‡æ¡£å’Œä¸Šä¸‹æ–‡")
    print("-" * 50)

    context = ProcessingContext(
        document_id="demo_doc_001",
        user_id="demo_user_zhang",
        session_id="session_20241201",
        metadata={
            "department": "æŠ€æœ¯éƒ¨",
            "project": "AIæ–‡æ¡£åˆ†æç³»ç»Ÿ",
            "priority": "high"
        }
    )

    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£å†…å®¹
    test_content = """
    æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿæ¶æ„è®¾è®¡

    1. ç³»ç»Ÿæ¦‚è¿°
    æœ¬ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„æ–‡æ¡£åˆ†æå¹³å°ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›æ™ºèƒ½åŒ–çš„æ–‡æ¡£å¤„ç†å’Œåˆ†ææœåŠ¡ã€‚

    2. æ ¸å¿ƒåŠŸèƒ½
    - æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆ
    - æƒ…æ„Ÿåˆ†æå’Œè§‚ç‚¹æå–
    - å…³é”®è¯è‡ªåŠ¨æå–
    - æ–‡æ¡£åˆ†ç±»å’Œæ ‡ç­¾

    3. æŠ€æœ¯æ¶æ„
    ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
    - APIç½‘å…³ï¼šè´Ÿè´£è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
    - æ–‡æ¡£å¤„ç†æœåŠ¡ï¼šè´Ÿè´£æ–‡æ¡£è§£æå’Œé¢„å¤„ç†
    - AIåˆ†ææœåŠ¡ï¼šé›†æˆDeepSeekå¤§è¯­è¨€æ¨¡å‹
    - ç¼“å­˜æœåŠ¡ï¼šæä¾›é«˜æ€§èƒ½æ•°æ®ç¼“å­˜

    4. éƒ¨ç½²æ–¹æ¡ˆ
    ç³»ç»Ÿæ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²ï¼Œä½¿ç”¨Dockerå’ŒKubernetesè¿›è¡ŒæœåŠ¡ç¼–æ’ã€‚
    æ•°æ®åº“é‡‡ç”¨åˆ†å¸ƒå¼é›†ç¾¤ï¼Œç¡®ä¿é«˜å¯ç”¨æ€§å’Œæ•°æ®ä¸€è‡´æ€§ã€‚

    5. æ€»ç»“
    æœ¬ç³»ç»Ÿé€šè¿‡å…ˆè¿›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›é«˜æ•ˆã€å‡†ç¡®çš„æ–‡æ¡£åˆ†ææœåŠ¡ï¼Œ
    å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§å’Œç»´æŠ¤æ€§ã€‚
    """

    print(f"ğŸ“‹ [æ–‡æ¡£ä¿¡æ¯] ID: {context.document_id}")
    print(f"ğŸ‘¤ [ç”¨æˆ·ä¿¡æ¯] ID: {context.user_id}")
    print(f"ğŸ“ [å†…å®¹ç»Ÿè®¡] é•¿åº¦: {len(test_content)} å­—ç¬¦, {len(test_content.split())} å•è¯")
    print(f"ğŸ·ï¸  [å…ƒæ•°æ®] éƒ¨é—¨: {context.metadata['department']}, é¡¹ç›®: {context.metadata['project']}")

    # æ­¥éª¤5: æ‰§è¡Œç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆæ— ç¼“å­˜ï¼‰
    print(f"\nğŸš€ [æ­¥éª¤5] ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆæ— ç¼“å­˜ï¼‰")
    print("=" * 60)
    print("ğŸ“Š [é¢„æœŸè¡Œä¸º] ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹")
    print("ğŸ”— [æ‰§è¡Œé“¾] æ—¥å¿—->ç¼“å­˜->æ€§èƒ½ç›‘æ§->é‡è¯•->AIåˆ†æ->åŸºç¡€å¤„ç†")
    print("=" * 60)

    start_time = time.time()
    result1 = enhanced_processor.process(test_content, context)
    end_time = time.time()
    total_time_1 = end_time - start_time

    print(f"\nğŸ“Š [ç¬¬ä¸€æ¬¡å¤„ç†] æ€»è€—æ—¶: {total_time_1:.3f} ç§’")

    # æ­¥éª¤6: æ‰§è¡Œç¬¬äºŒæ¬¡å¤„ç†ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\nğŸ¯ [æ­¥éª¤6] ç¬¬äºŒæ¬¡å¤„ç†ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰")
    print("=" * 60)
    print("ğŸ“Š [é¢„æœŸè¡Œä¸º] ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡AIåˆ†æç›´æ¥è¿”å›ç»“æœ")
    print("ğŸ”— [æ‰§è¡Œé“¾] æ—¥å¿—->ç¼“å­˜ï¼ˆå‘½ä¸­ï¼‰->è¿”å›")
    print("=" * 60)

    start_time = time.time()
    result2 = enhanced_processor.process(test_content, context)
    end_time = time.time()
    total_time_2 = end_time - start_time

    print(f"\nğŸ“Š [ç¬¬äºŒæ¬¡å¤„ç†] æ€»è€—æ—¶: {total_time_2:.3f} ç§’")
    print(f"âš¡ [æ€§èƒ½æå‡] ç¼“å­˜åŠ é€Ÿ: {((total_time_1 - total_time_2) / total_time_1 * 100):.1f}%")

    # æ­¥éª¤7: å±•ç¤ºè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ [æ­¥éª¤7] è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)

    # ç¼“å­˜ç»Ÿè®¡
    print(f"ğŸ’¾ [ç¼“å­˜ç»Ÿè®¡]")
    cache_stats = cache_processor.get_cache_stats()
    formatted_cache_stats = json.dumps(cache_stats, indent=2, ensure_ascii=False)
    for line in formatted_cache_stats.split('\n'):
        print(f"   {line}")

    # æ€§èƒ½ç»Ÿè®¡
    print(f"\nâ±ï¸  [æ€§èƒ½ç»Ÿè®¡]")
    perf_report = performance_processor.get_performance_report()
    formatted_perf_stats = json.dumps(perf_report, indent=2, ensure_ascii=False)
    for line in formatted_perf_stats.split('\n'):
        print(f"   {line}")

    # AIåˆ†æç»Ÿè®¡
    print(f"\nğŸ¤– [AIåˆ†æç»Ÿè®¡]")
    ai_stats = ai_enhanced_processor.get_ai_stats()
    formatted_ai_stats = json.dumps(ai_stats, indent=2, ensure_ascii=False)
    for line in formatted_ai_stats.split('\n'):
        print(f"   {line}")

    # æ—¥å¿—ç»Ÿè®¡
    print(f"\nğŸ“ [æ—¥å¿—ç»Ÿè®¡]")
    log_summary = enhanced_processor.get_log_summary()
    formatted_log_stats = json.dumps(log_summary, indent=2, ensure_ascii=False)
    for line in formatted_log_stats.split('\n'):
        print(f"   {line}")

    # æ­¥éª¤8: è£…é¥°å™¨æ¨¡å¼æ€»ç»“
    print(f"\nğŸ“ [æ­¥éª¤8] è£…é¥°å™¨æ¨¡å¼æ€»ç»“")
    print("=" * 80)
    print("""
ğŸ­ è£…é¥°å™¨æ¨¡å¼æ ¸å¿ƒè¦ç‚¹ï¼š

1. ğŸ­ Component (ç»„ä»¶)
   - DocumentProcessor: å®šä¹‰åŸºç¡€å¤„ç†æ¥å£
   - æ‰€æœ‰è£…é¥°å™¨å’Œå…·ä½“ç»„ä»¶éƒ½å®ç°ç›¸åŒæ¥å£

2. ğŸ­ Decorator (è£…é¥°å™¨æŠ½è±¡ç±»)
   - ProcessorDecorator: æŒæœ‰ç»„ä»¶å¼•ç”¨ï¼Œå®šä¹‰è£…é¥°å™¨æ¥å£
   - é€šè¿‡å§”æ‰˜æœºåˆ¶è°ƒç”¨è¢«è£…é¥°å¯¹è±¡

3. ğŸ”§ Concrete Decorator (å…·ä½“è£…é¥°å™¨)
   - LoggingDecorator: æ·»åŠ æ—¥å¿—è®°å½•åŠŸèƒ½
   - CacheDecorator: æ·»åŠ ç¼“å­˜åŠŸèƒ½
   - PerformanceMonitorDecorator: æ·»åŠ æ€§èƒ½ç›‘æ§
   - RetryDecorator: æ·»åŠ é‡è¯•æœºåˆ¶
   - AIEnhancementDecorator: æ·»åŠ AIåˆ†æåŠŸèƒ½

4. ğŸ”„ åŠ¨æ€ç»„åˆ
   - å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ¨æ€ç»„åˆè£…é¥°å™¨
   - æ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç å³å¯æ·»åŠ æ–°åŠŸèƒ½
   - è£…é¥°å™¨é¡ºåºå¯ä»¥çµæ´»è°ƒæ•´

5. ğŸ“Š é€æ˜æ€§
   - è£…é¥°å™¨å¯¹å®¢æˆ·ç«¯é€æ˜
   - è£…é¥°åçš„å¯¹è±¡ä¸åŸå§‹å¯¹è±¡æ¥å£ä¸€è‡´

è£…é¥°å™¨æ¨¡å¼çš„ä¼˜åŠ¿ï¼š
âœ… åŠ¨æ€æ·»åŠ åŠŸèƒ½ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
âœ… å¯ä»¥ç»„åˆå¤šä¸ªè£…é¥°å™¨å®ç°å¤æ‚åŠŸèƒ½
âœ… ç¬¦åˆå¼€é—­åŸåˆ™ï¼Œå¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å°é—­
âœ… æ¯”ç»§æ‰¿æ›´çµæ´»ï¼Œé¿å…ç±»çˆ†ç‚¸
âœ… å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ¨æ€æ·»åŠ æˆ–ç§»é™¤åŠŸèƒ½

å®é™…åº”ç”¨åœºæ™¯ï¼š
- AIå·¥ä½œæµä¸­çš„åŠŸèƒ½å¢å¼º
- Webæ¡†æ¶ä¸­çš„ä¸­é—´ä»¶
- IOæµçš„å¢å¼ºå¤„ç†
- GUIç»„ä»¶çš„åŠŸèƒ½æ‰©å±•
- ç¼“å­˜ã€æ—¥å¿—ã€æƒé™ç­‰æ¨ªåˆ‡å…³æ³¨ç‚¹
""")

    return enhanced_processor


if __name__ == "__main__":
    # è¿è¡Œè£…é¥°å™¨æ¨¡å¼å®Œæ•´æ¼”ç¤º
    enhanced_processor = demo_decorator_pattern()