"""
ç­–ç•¥æ¨¡å¼ - æ–‡æ¡£å¤„ç†ç­–ç•¥

ç­–ç•¥æ¨¡å¼å…è®¸åœ¨è¿è¡Œæ—¶é€‰æ‹©ç®—æ³•çš„è¡Œä¸ºã€‚åœ¨AIå·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸åŒçš„æ–‡æ¡£å¤„ç†ç­–ç•¥ï¼Œ
æ ¹æ®æ–‡æ¡£ç±»å‹ã€é•¿åº¦æˆ–å†…å®¹ç‰¹ç‚¹é€‰æ‹©æœ€åˆé€‚çš„å¤„ç†æ–¹æ³•ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- å®šä¹‰ç®—æ³•æ—ï¼ˆä¸åŒçš„æ–‡æ¡£å¤„ç†ç­–ç•¥ï¼‰
- å°è£…æ¯ç§ç®—æ³•ï¼ˆå…·ä½“çš„å¤„ç†ç­–ç•¥å®ç°ï¼‰
- åœ¨è¿è¡Œæ—¶é€‰æ‹©åˆé€‚çš„ç®—æ³•ï¼ˆæ ¹æ®æ–‡æ¡£ç±»å‹ï¼‰
- å®¢æˆ·ä»£ç é€šè¿‡ç­–ç•¥æ¥å£ä¸ç®—æ³•äº¤äº’ï¼Œæ— éœ€äº†è§£å…·ä½“å®ç°

é€‚ç”¨åœºæ™¯ï¼š
- AIæ–‡æ¡£åˆ†æç³»ç»Ÿä¸­ä¸åŒç±»å‹æ–‡æ¡£çš„å¤„ç†
- å†…å®¹åˆ†ç±»å¤„ç†
- æ•°æ®é¢„å¤„ç†æµç¨‹ä¸­çš„å¤šç§ç®—æ³•é€‰æ‹©

ä¼˜åŠ¿ï¼š
- å¼€é—­-å°é—­åŸåˆ™ï¼šå¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å°é—­
- é¿å…å¤šé‡æ¡ä»¶è¯­å¥
- æé«˜ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¤ç”¨æ€§
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any
import sys
import os
import time
import json
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ai_service import get_ai_service
from dataclasses import dataclass


@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    content: str
    title: str = ""
    doc_type: str = "general"  # legal, technical, academic, general
    length: int = 0

    def __post_init__(self):
        self.length = len(self.content)


class DocumentProcessingStrategy(ABC):
    """
    æ–‡æ¡£å¤„ç†ç­–ç•¥æŠ½è±¡åŸºç±»

    è¿™æ˜¯ç­–ç•¥æ¨¡å¼ä¸­çš„"ç­–ç•¥æ¥å£"ï¼Œå®šä¹‰äº†æ‰€æœ‰å…·ä½“ç­–ç•¥å¿…é¡»å®ç°çš„æ–¹æ³•ã€‚

    è®¾è®¡åŸåˆ™ï¼š
    - æ¯ä¸ªå…·ä½“ç­–ç•¥éƒ½å®ç°ç›¸åŒçš„æ¥å£
    - æ¥å£æ–¹æ³•åº”è¯¥å…·æœ‰æ¸…æ™°çš„è¯­ä¹‰
    - å‚æ•°å’Œè¿”å›å€¼ç±»å‹åº”è¯¥ä¿æŒä¸€è‡´
    """

    @abstractmethod
    def process(self, document: Document) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£çš„æ ¸å¿ƒæ–¹æ³•

        Args:
            document: å¾…å¤„ç†çš„æ–‡æ¡£å¯¹è±¡

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœï¼ŒåŒ…å«ç­–ç•¥ç±»å‹ã€åˆ†å—æ•°ã€å¤„ç†ç‰‡æ®µç­‰

        æ³¨æ„ï¼šæ¯ä¸ªç­–ç•¥çš„å…·ä½“å®ç°éƒ½ä¸åŒï¼Œä½†è¿”å›æ ¼å¼åº”è¯¥ä¿æŒä¸€è‡´
        """
        pass

    @abstractmethod
    def get_chunk_size(self, document: Document) -> int:
        """
        è·å–æ¨èçš„åˆ†å—å¤§å°

        Args:
            document: å¾…å¤„ç†çš„æ–‡æ¡£å¯¹è±¡

        Returns:
            int: æ¨èçš„åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

        è¯´æ˜ï¼šä¸åŒç±»å‹çš„æ–‡æ¡£å¯èƒ½éœ€è¦ä¸åŒçš„åˆ†å—ç­–ç•¥
        - æ³•å¾‹æ–‡æ¡£ï¼šå°å—ï¼ˆä¿æŒæ¡æ¬¾å®Œæ•´æ€§ï¼‰
        - æŠ€æœ¯æ–‡æ¡£ï¼šä¸­ç­‰å—ï¼ˆå¯ä»¥åŒ…å«ä»£ç ï¼‰
        - å­¦æœ¯æ–‡æ¡£ï¼šå¤§å—ï¼ˆä¿æŒè®ºè¯å®Œæ•´æ€§ï¼‰
        """
        pass


class LegalDocumentStrategy(DocumentProcessingStrategy):
    """
    æ³•å¾‹æ–‡æ¡£å¤„ç†ç­–ç•¥ - å…·ä½“ç­–ç•¥å®ç°

    è¿™æ˜¯ç­–ç•¥æ¨¡å¼ä¸­çš„"å…·ä½“ç­–ç•¥"ä¹‹ä¸€ï¼Œä¸“é—¨å¤„ç†æ³•å¾‹ç±»æ–‡æ¡£ã€‚

    å¤„ç†ç‰¹ç‚¹ï¼š
    - ä¸“æ³¨æ³•å¾‹æ¡æ¬¾ã€åˆåŒå†…å®¹ã€æ³•è§„æ–‡æœ¬
    - æå–å…³é”®æ³•å¾‹æ¦‚å¿µã€å½“äº‹äººã€æ—¶é—´åœ°ç‚¹
    - è¯†åˆ«æ³•å¾‹è´£ä»»å’Œä¹‰åŠ¡
    - å°å—åˆ†å‰²ä»¥ä¿æŒæ¡æ¬¾å®Œæ•´æ€§
    """

    def process(self, document: Document) -> Dict[str, Any]:
        """
        å¤„ç†æ³•å¾‹æ–‡æ¡£çš„ä¸»æµç¨‹

        å¤„ç†æ­¥éª¤ï¼š
        1. è·å–é€‚åˆæ³•å¾‹æ–‡æ¡£çš„åˆ†å—å¤§å°
        2. æŒ‰æ®µè½åˆ†å‰²æ–‡æ¡£ï¼ˆä¿æŒæ¡æ¬¾å®Œæ•´æ€§ï¼‰
        3. å¯¹æ¯ä¸ªåˆ†å—è°ƒç”¨AIåˆ†æ
        4. æ±‡æ€»å¤„ç†ç»“æœ
        """
        print(f"ğŸ“‹ [ç­–ç•¥å¯åŠ¨] ä½¿ç”¨æ³•å¾‹æ–‡æ¡£ç­–ç•¥å¤„ç†: {document.title}")
        print(f"ğŸ“ [åˆ†å—é…ç½®] è·å–æ³•å¾‹æ–‡æ¡£æ¨èçš„åˆ†å—å¤§å°...")

        chunk_size = self.get_chunk_size(document)
        print(f"ğŸ“Š [åˆ†å—å¤§å°] {chunk_size} å­—ç¬¦")

        print(f"âœ‚ï¸  [æ–‡æ¡£åˆ†å—] å¼€å§‹åˆ†å‰²æ–‡æ¡£...")
        chunks = self._split_document(document.content, chunk_size)
        print(f"ğŸ”¢ [åˆ†å—ç»“æœ] å…±åˆ†å‰²ä¸º {len(chunks)} ä¸ªå¤„ç†å—")

        print(f"ğŸ¤– [AIåˆ†æ] å¼€å§‹å¯¹æ¯ä¸ªåˆ†å—è¿›è¡ŒAIåˆ†æ...")
        processed_chunks = []

        for i, chunk in enumerate(chunks):
            print(f"\nğŸ“„ [å¤„ç†åˆ†å— {i+1}/{len(chunks)}]")
            print(f"ğŸ“ [åˆ†å—é¢„è§ˆ] {chunk[:80]}..." if len(chunk) > 80 else f"ğŸ“ [åˆ†å—å†…å®¹] {chunk}")

            # æ„å»ºä¸“é—¨ç”¨äºæ³•å¾‹æ–‡æ¡£çš„åˆ†ææç¤ºè¯
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ³•å¾‹æ–‡æ¡£ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

            ç‰‡æ®µ {i+1}/{len(chunks)}:
            {chunk}

            è¯·æå–ï¼š
            1. å…³é”®æ³•å¾‹æ¡æ¬¾
            2. ç›¸å…³æ³•å¾‹æ¦‚å¿µ
            3. é‡è¦çš„æ—¶é—´ã€åœ°ç‚¹ã€å½“äº‹äºº
            4. æ³•å¾‹è´£ä»»å’Œä¹‰åŠ¡

            ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
            """

            # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šè°ƒç”¨DeepSeek APIè¿›è¡Œåˆ†æ
            print(f"ğŸŒ [APIè°ƒç”¨] è°ƒç”¨DeepSeek APIè¿›è¡Œæ³•å¾‹æ–‡æ¡£åˆ†æ...")
            try:
                ai_service = get_ai_service("deepseek")
                analysis_result = ai_service.analyze_document(chunk, "legal")

                if analysis_result["success"]:
                    print(f"âœ… [APIæˆåŠŸ] ç¬¬{i+1}ä¸ªåˆ†å—åˆ†ææˆåŠŸ")
                    usage = analysis_result.get("usage", {})
                    if usage:
                        print(f"ğŸ“Š [Tokenä½¿ç”¨] è¾“å…¥:{usage.get('prompt_tokens', 0)} è¾“å‡º:{usage.get('completion_tokens', 0)} æ€»è®¡:{usage.get('total_tokens', 0)}")

                    # ğŸ”¥ æ–°å¢ï¼šæ‰“å°AIè¿”å›çš„JSONç»“æ„åŒ–æ•°æ®
                    self._print_ai_analysis_result(analysis_result, i + 1)

                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": analysis_result["content"],
                        "raw_response": analysis_result,  # ä¿å­˜å®Œæ•´çš„AIå“åº”
                        "usage": usage,
                        "status": "success"
                    })
                else:
                    # APIè°ƒç”¨å¤±è´¥æ—¶çš„é™çº§å¤„ç†
                    print(f"âŒ [APIå¤±è´¥] ç¬¬{i+1}ä¸ªåˆ†å—åˆ†æå¤±è´¥: {analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": "æ³•å¾‹æ¡æ¬¾åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†",
                        "error": analysis_result.get("error", "æœªçŸ¥é”™è¯¯"),
                        "status": "api_failed"
                    })
            except Exception as e:
                # å‡ºé”™æ—¶çš„å¤„ç†
                print(f"ğŸ’¥ [å¼‚å¸¸å¤„ç†] ç¬¬{i+1}ä¸ªåˆ†å—å¤„ç†å¼‚å¸¸: {str(e)}")
                processed_chunks.append({
                    "chunk_id": i,
                    "content_preview": chunk[:100] + "...",
                    "analysis": "æ³•å¾‹æ¡æ¬¾åˆ†æç»“æœï¼ˆé™çº§æ¨¡å¼ï¼‰",
                    "fallback": True,
                    "error": str(e),
                    "status": "exception"
                })

        # ğŸ“Š ç»Ÿè®¡å¤„ç†ç»“æœå¹¶è¿”å›
        success_count = sum(1 for chunk in processed_chunks if chunk.get("status") == "success")
        print(f"\nğŸ“ˆ [å¤„ç†ç»Ÿè®¡] æˆåŠŸå¤„ç† {success_count}/{len(chunks)} ä¸ªåˆ†å—")

        result = {
            "strategy": "legal",
            "total_chunks": len(chunks),
            "processed_chunks": processed_chunks,
            "success_rate": success_count / len(chunks) if chunks else 0,
            "key_points": [f"æ³•å¾‹è¦ç‚¹ {i+1}" for i in range(min(5, len(chunks)))],
            "processing_summary": {
                "total_successful": success_count,
                "total_failed": len(chunks) - success_count,
                "fallback_used": any(chunk.get("fallback") for chunk in processed_chunks)
            }
        }

        print(f"ğŸ¯ [ç­–ç•¥å®Œæˆ] æ³•å¾‹æ–‡æ¡£ç­–ç•¥å¤„ç†å®Œæˆ")
        print(f"ğŸ“‹ [ç»“æœæ‘˜è¦] æˆåŠŸç‡: {result['processing_summary']['total_successful']}/{result['total_chunks']} ({result['success_rate']:.1%})")

        return result

    def get_chunk_size(self, document: Document) -> int:
        """
        è·å–æ³•å¾‹æ–‡æ¡£æ¨èçš„åˆ†å—å¤§å°

        è®¾è®¡ç†ç”±ï¼š
        - æ³•å¾‹æ–‡æ¡£éœ€è¦ä¿æŒæ¡æ¬¾çš„å®Œæ•´æ€§
        - è¾ƒå°çš„åˆ†å—é¿å…ç ´åæ³•å¾‹é€»è¾‘
        - ä¾¿äºAIç²¾ç¡®è¯†åˆ«æ³•å¾‹æ¦‚å¿µ
        """
        print(f"ğŸ” [åˆ†å—ç­–ç•¥] æ³•å¾‹æ–‡æ¡£æ¨èåˆ†å—å¤§å°: 2000å­—ç¬¦")
        print(f"ğŸ’¡ [è®¾è®¡ç†ç”±] ä¿æŒæ³•å¾‹æ¡æ¬¾å®Œæ•´æ€§ï¼Œé¿å…ç ´åæ³•å¾‹é€»è¾‘")
        return 2000

    def _split_document(self, content: str, chunk_size: int) -> List[str]:
        """
        æŒ‰æ®µè½åˆ†å‰²æ–‡æ¡£ï¼Œä¿æŒæ³•å¾‹æ¡æ¬¾å®Œæ•´æ€§

        åˆ†å‰²ç­–ç•¥ï¼š
        - ä¼˜å…ˆæŒ‰æ®µè½åˆ†éš”ç¬¦('\n\n')åˆ†å‰²
        - é¿å…åœ¨æ®µè½ä¸­é—´åˆ†å‰²
        - ç¡®ä¿æ¯ä¸ªåˆ†å—éƒ½åŒ…å«å®Œæ•´çš„æ³•å¾‹æ®µè½

        Args:
            content: æ–‡æ¡£å†…å®¹
            chunk_size: åˆ†å—å¤§å°

        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        print(f"âœ‚ï¸  [åˆ†å‰²ç®—æ³•] ä½¿ç”¨æ®µè½åˆ†å‰²ç­–ç•¥")
        print(f"ğŸ“ [åˆ†å‰²æ ‡è®°] æŒ‰ '\\n\\n' (æ®µè½åˆ†éš”ç¬¦)è¿›è¡Œåˆ†å‰²")

        paragraphs = content.split('\n\n')
        print(f"ğŸ“Š [æ®µè½ç»Ÿè®¡] å…± {len(paragraphs)} ä¸ªæ®µè½")

        chunks = []
        current_chunk = ""
        current_length = 0

        for i, paragraph in enumerate(paragraphs):
            paragraph_len = len(paragraph) + 2  # +2 for \n\n
            print(f"ğŸ“„ [æ®µè½{i+1}] é•¿åº¦: {paragraph_len} å­—ç¬¦")

            if current_length + paragraph_len <= chunk_size:
                # å¯ä»¥æ·»åŠ åˆ°å½“å‰åˆ†å—
                current_chunk += paragraph + "\n\n"
                current_length += paragraph_len
                print(f"  â¡ï¸  æ·»åŠ åˆ°å½“å‰åˆ†å— (å½“å‰æ€»é•¿: {current_length})")
            else:
                # éœ€è¦å¼€å§‹æ–°çš„åˆ†å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    print(f"  ğŸ“¦ [åˆ†å—å®Œæˆ] ç”Ÿæˆç¬¬{len(chunks)}ä¸ªåˆ†å—ï¼Œé•¿åº¦: {len(current_chunk)}")
                current_chunk = paragraph + "\n\n"
                current_length = paragraph_len
                print(f"  ğŸ†• [æ–°åˆ†å—] å¼€å§‹æ–°çš„åˆ†å—ï¼Œé•¿åº¦: {current_length}")

        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            chunks.append(current_chunk.strip())
            print(f"  ğŸ“¦ [æœ€ç»ˆåˆ†å—] ç”Ÿæˆç¬¬{len(chunks)}ä¸ªåˆ†å—ï¼Œé•¿åº¦: {len(current_chunk)}")

        print(f"âœ… [åˆ†å‰²å®Œæˆ] å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
        return chunks

    def _print_ai_analysis_result(self, analysis_result: Dict[str, Any], chunk_num: int):
        """
        ğŸ“Š æ‰“å°AIåˆ†æè¿”å›çš„JSONç»“æ„åŒ–æ•°æ®

        è¿™ä¸ªæ–¹æ³•ä¸“é—¨ç”¨äºå±•ç¤ºAIåˆ†æç»“æœçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - åŸå§‹JSONå“åº”
        - è§£æåçš„ç»“æ„åŒ–æ•°æ®
        - å…³é”®ä¿¡æ¯æå–

        Args:
            analysis_result: AIåˆ†æçš„å®Œæ•´å“åº”ç»“æœ
            chunk_num: å½“å‰åˆ†å—ç¼–å·
        """
        print(f"\nğŸ“‹ [AIåˆ†æç»“æœ] ç¬¬{chunk_num}ä¸ªåˆ†å—çš„è¯¦ç»†åˆ†æç»“æœ:")
        print("=" * 50)

        try:
            # 1. æ‰“å°å®Œæ•´çš„åŸå§‹å“åº”ç»“æ„
            print(f"ğŸ—‚ï¸  [å®Œæ•´å“åº”] AIè¿”å›çš„å®Œæ•´ç»“æ„:")
            formatted_response = json.dumps(analysis_result, indent=2, ensure_ascii=False)
            print(formatted_response)

            # 2. æå–å¹¶æ‰“å°åˆ†æå†…å®¹
            if "content" in analysis_result:
                print(f"\nğŸ“ [åˆ†æå†…å®¹] AIç”Ÿæˆçš„æ³•å¾‹åˆ†æ:")
                print("-" * 30)
                content = analysis_result["content"]

                # å°è¯•è§£æJSONæ ¼å¼çš„å†…å®¹
                if isinstance(content, str):
                    try:
                        # å°è¯•å°†å†…å®¹è§£æä¸ºJSON
                        parsed_content = json.loads(content)
                        print(f"âœ… [JSONè§£ææˆåŠŸ] å†…å®¹ä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼:")
                        print(json.dumps(parsed_content, indent=2, ensure_ascii=False))

                        # æå–å…³é”®å­—æ®µ
                        if isinstance(parsed_content, dict):
                            print(f"\nğŸ” [å…³é”®å­—æ®µæå–]:")
                            for key, value in parsed_content.items():
                                print(f"  ğŸ“Œ {key}: {value}")
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥æ‰“å°æ–‡æœ¬å†…å®¹
                        print(f"ğŸ“„ [æ–‡æœ¬å†…å®¹] AIåˆ†æç»“æœ:")
                        print(content)
                else:
                    print(f"ğŸ“Š [æ•°æ®å†…å®¹] {content}")

            # 3. æ‰“å°å…ƒæ•°æ®ä¿¡æ¯
            print(f"\nğŸ“Š [å…ƒæ•°æ®ä¿¡æ¯]:")

            # APIä¿¡æ¯
            if "api_info" in analysis_result:
                api_info = analysis_result["api_info"]
                print(f"  ğŸŒ APIä¿¡æ¯:")
                print(f"    æ¨¡å‹: {api_info.get('model', 'Unknown')}")
                print(f"    çŠ¶æ€: {api_info.get('status', 'Unknown')}")

            # Tokenä½¿ç”¨æƒ…å†µ
            if "usage" in analysis_result:
                usage = analysis_result["usage"]
                print(f"  ğŸ’° Tokenç»Ÿè®¡:")
                print(f"    è¾“å…¥Token: {usage.get('prompt_tokens', 0)}")
                print(f"    è¾“å‡ºToken: {usage.get('completion_tokens', 0)}")
                print(f"    æ€»è®¡Token: {usage.get('total_tokens', 0)}")

                # è®¡ç®—æˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾æ¯1000ä¸ªtokençº¦0.01å…ƒï¼‰
                total_tokens = usage.get('total_tokens', 0)
                estimated_cost = (total_tokens / 1000) * 0.01
                print(f"    ğŸ’µ é¢„ä¼°æˆæœ¬: Â¥{estimated_cost:.4f}")

            # å¤„ç†æ—¶é—´ä¿¡æ¯
            if "timestamp" in analysis_result:
                timestamp = analysis_result["timestamp"]
                print(f"  â° å¤„ç†æ—¶é—´: {timestamp}")

            # 4. åˆ†æè´¨é‡è¯„ä¼°
            print(f"\nğŸ¯ [è´¨é‡è¯„ä¼°]:")
            content_length = len(str(analysis_result.get("content", "")))
            print(f"  ğŸ“ å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")

            if content_length > 500:
                print(f"  âœ… å†…å®¹è¯¦ç»†åº¦: è¯¦ç»†")
            elif content_length > 200:
                print(f"  âš ï¸  å†…å®¹è¯¦ç»†åº¦: ä¸­ç­‰")
            else:
                print(f"  âŒ å†…å®¹è¯¦ç»†åº¦: ç®€ç•¥")

            # 5. JSONæ ¼å¼éªŒè¯
            print(f"\nğŸ” [æ ¼å¼éªŒè¯]:")
            content = analysis_result.get("content", "")
            if isinstance(content, str):
                try:
                    json.loads(content)
                    print(f"  âœ… JSONæ ¼å¼: æœ‰æ•ˆ")
                except json.JSONDecodeError as e:
                    print(f"  âŒ JSONæ ¼å¼: æ— æ•ˆ ({e})")
            else:
                print(f"  ğŸ“Š æ•°æ®ç±»å‹: {type(content).__name__}")

        except Exception as e:
            print(f"âŒ [è§£æé”™è¯¯] æ— æ³•è§£æAIè¿”å›ç»“æœ: {str(e)}")
            print(f"ğŸ“„ [åŸå§‹æ•°æ®] {analysis_result}")

        print("=" * 50)
        print(f"ğŸ [åˆ†æå®Œæˆ] ç¬¬{chunk_num}ä¸ªåˆ†å—åˆ†æç»“æœå±•ç¤ºå®Œæ¯•\n")


class TechnicalDocumentStrategy(DocumentProcessingStrategy):
    """æŠ€æœ¯æ–‡æ¡£å¤„ç†ç­–ç•¥"""

    def process(self, document: Document) -> Dict[str, Any]:
        """å¤„ç†æŠ€æœ¯æ–‡æ¡£ - ä¸“æ³¨æŠ€æœ¯æ¦‚å¿µå’Œä»£ç è§£é‡Š"""
        print(f"ğŸ’» ä½¿ç”¨æŠ€æœ¯æ–‡æ¡£ç­–ç•¥å¤„ç†: {document.title}")

        chunk_size = self.get_chunk_size(document)
        chunks = self._split_document(document.content, chunk_size)

        processed_chunks = []
        for i, chunk in enumerate(chunks):
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ç‰‡æ®µï¼š

            ç‰‡æ®µ {i+1}/{len(chunks)}:
            {chunk}

            è¯·æå–ï¼š
            1. å…³é”®æŠ€æœ¯æ¦‚å¿µ
            2. ä»£ç ç‰‡æ®µå’ŒåŠŸèƒ½è¯´æ˜
            3. æŠ€æœ¯æ¶æ„å’Œæµç¨‹
            4. APIæ¥å£å’Œå‚æ•°è¯´æ˜

            ä»¥Markdownæ ¼å¼è¿”å›ç»“æœã€‚
            """

            # è°ƒç”¨DeepSeek APIè¿›è¡ŒæŠ€æœ¯æ–‡æ¡£åˆ†æ
            try:
                ai_service = get_ai_service("deepseek")
                analysis_result = ai_service.analyze_document(chunk, "technical")

                if analysis_result["success"]:
                    print(f"âœ… [æŠ€æœ¯åˆ†ææˆåŠŸ] ç¬¬{i+1}ä¸ªåˆ†å—åˆ†æå®Œæˆ")
                    # æ‰“å°AIè¿”å›çš„è¯¦ç»†ç»“æœ
                    self._print_technical_analysis_result(analysis_result, i + 1)

                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": analysis_result["content"],
                        "raw_response": analysis_result,
                        "usage": analysis_result.get("usage", {})
                    })
                else:
                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": "æŠ€æœ¯æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†",
                        "error": analysis_result.get("error", "æœªçŸ¥é”™è¯¯")
                    })
            except Exception as e:
                processed_chunks.append({
                    "chunk_id": i,
                    "content_preview": chunk[:100] + "...",
                    "analysis": "æŠ€æœ¯æ–‡æ¡£åˆ†æç»“æœï¼ˆé™çº§æ¨¡å¼ï¼‰",
                    "fallback": True,
                    "error": str(e)
                })

        return {
            "strategy": "technical",
            "total_chunks": len(chunks),
            "processed_chunks": processed_chunks,
            "key_concepts": [f"æŠ€æœ¯æ¦‚å¿µ {i+1}" for i in range(min(5, len(chunks)))]
        }

    def get_chunk_size(self, document: Document) -> int:
        """æŠ€æœ¯æ–‡æ¡£å¯ä»¥åŒ…å«ä»£ç å—ï¼Œä½¿ç”¨ä¸­ç­‰å¤§å°çš„åˆ†å—"""
        return 3000

    def _split_document(self, content: str, chunk_size: int) -> List[str]:
        """æŒ‰ä»£ç å—å’Œæ®µè½åˆ†å‰²æŠ€æœ¯æ–‡æ¡£"""
        sections = content.split('\n# ')
        chunks = []
        current_chunk = ""

        for section in sections:
            # é‡æ–°æ·»åŠ #å·ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªsectionï¼‰
            if section != sections[0]:
                section = '#' + section

            if len(current_chunk) + len(section) <= chunk_size:
                current_chunk += section
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = section

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks or [content]  # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªchunk

    def _print_technical_analysis_result(self, analysis_result: Dict[str, Any], chunk_num: int):
        """
        ğŸ”§ æ‰“å°æŠ€æœ¯æ–‡æ¡£AIåˆ†æçš„JSONç»“æ„åŒ–æ•°æ®

        ä¸“é—¨å±•ç¤ºæŠ€æœ¯æ–‡æ¡£åˆ†æç»“æœï¼ŒåŒ…æ‹¬ä»£ç è¯†åˆ«ã€æŠ€æœ¯æ¦‚å¿µæå–ç­‰

        Args:
            analysis_result: AIåˆ†æçš„å®Œæ•´å“åº”ç»“æœ
            chunk_num: å½“å‰åˆ†å—ç¼–å·
        """
        print(f"\nğŸ’» [æŠ€æœ¯åˆ†æç»“æœ] ç¬¬{chunk_num}ä¸ªåˆ†å—çš„è¯¦ç»†æŠ€æœ¯åˆ†æ:")
        print("=" * 50)

        try:
            # 1. æ‰“å°å®Œæ•´å“åº”
            print(f"ğŸ—‚ï¸  [å®Œæ•´å“åº”] AIè¿”å›çš„æŠ€æœ¯åˆ†æç»“æ„:")
            formatted_response = json.dumps(analysis_result, indent=2, ensure_ascii=False)
            print(formatted_response)

            # 2. æŠ€æœ¯å†…å®¹åˆ†æ
            if "content" in analysis_result:
                print(f"\nğŸ”§ [æŠ€æœ¯å†…å®¹] AIæå–çš„æŠ€æœ¯ä¿¡æ¯:")
                print("-" * 30)
                content = analysis_result["content"]

                if isinstance(content, str):
                    try:
                        # å°è¯•è§£æJSONæ ¼å¼
                        parsed_content = json.loads(content)
                        print(f"âœ… [JSONè§£ææˆåŠŸ] æŠ€æœ¯åˆ†æä¸ºJSONæ ¼å¼:")
                        print(json.dumps(parsed_content, indent=2, ensure_ascii=False))

                        # æŠ€æœ¯ç‰¹å®šå­—æ®µæå–
                        if isinstance(parsed_content, dict):
                            print(f"\nğŸ” [æŠ€æœ¯è¦ç´ æå–]:")
                            tech_keywords = ['concepts', 'apis', 'code', 'architecture', 'parameters']
                            for key, value in parsed_content.items():
                                if any(keyword in key.lower() for keyword in tech_keywords):
                                    print(f"  âš™ï¸  {key}: {value}")
                                else:
                                    print(f"  ğŸ“Œ {key}: {value}")
                    except json.JSONDecodeError:
                        print(f"ğŸ“„ [æŠ€æœ¯æ–‡æ¡£] åˆ†æç»“æœ:")
                        print(content)

                        # ç®€å•çš„æŠ€æœ¯å…³é”®è¯æœç´¢
                        tech_indicators = ['API', 'function', 'class', 'method', 'parameter', 'code', 'algorithm']
                        found_keywords = [indicator for indicator in tech_indicators if indicator.lower() in content.lower()]
                        if found_keywords:
                            print(f"ğŸ” [è¯†åˆ«çš„æŠ€æœ¯æŒ‡æ ‡] {', '.join(found_keywords)}")
                else:
                    print(f"ğŸ“Š [æ•°æ®å†…å®¹] {content}")

            # 3. å…ƒæ•°æ®å’ŒæŠ€æœ¯ç»Ÿè®¡
            print(f"\nğŸ“Š [æŠ€æœ¯å…ƒæ•°æ®]:")

            if "usage" in analysis_result:
                usage = analysis_result["usage"]
                print(f"  ğŸ’° Tokenä½¿ç”¨:")
                print(f"    è¾“å…¥: {usage.get('prompt_tokens', 0)} | è¾“å‡º: {usage.get('completion_tokens', 0)} | æ€»è®¡: {usage.get('total_tokens', 0)}")

            # 4. æŠ€æœ¯åˆ†æè´¨é‡è¯„ä¼°
            content_length = len(str(analysis_result.get("content", "")))
            print(f"  ğŸ“ æŠ€æœ¯æ–‡æ¡£é•¿åº¦: {content_length} å­—ç¬¦")

            # æ£€æµ‹æ˜¯å¦åŒ…å«ä»£ç å—
            content = str(analysis_result.get("content", ""))
            has_code_block = '```' in content or '`' in content
            print(f"  ğŸ–¥ï¸  åŒ…å«ä»£ç å—: {'æ˜¯' if has_code_block else 'å¦'}")

        except Exception as e:
            print(f"âŒ [æŠ€æœ¯åˆ†æé”™è¯¯] æ— æ³•è§£ææŠ€æœ¯åˆ†æç»“æœ: {str(e)}")

        print("=" * 50)
        print(f"ğŸ [æŠ€æœ¯åˆ†æå®Œæˆ] ç¬¬{chunk_num}ä¸ªæŠ€æœ¯åˆ†æå±•ç¤ºå®Œæ¯•\n")


class AcademicDocumentStrategy(DocumentProcessingStrategy):
    """å­¦æœ¯æ–‡æ¡£å¤„ç†ç­–ç•¥"""

    def process(self, document: Document) -> Dict[str, Any]:
        """å¤„ç†å­¦æœ¯æ–‡æ¡£ - ä¸“æ³¨ç ”ç©¶æ–¹æ³•å’Œç»“è®ºæå–"""
        print(f"ğŸ“ ä½¿ç”¨å­¦æœ¯æ–‡æ¡£ç­–ç•¥å¤„ç†: {document.title}")

        chunk_size = self.get_chunk_size(document)
        chunks = self._split_document(document.content, chunk_size)

        processed_chunks = []
        for i, chunk in enumerate(chunks):
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹å­¦æœ¯æ–‡æ¡£ç‰‡æ®µï¼š

            ç‰‡æ®µ {i+1}/{len(chunks)}:
            {chunk}

            è¯·æå–ï¼š
            1. ç ”ç©¶é—®é¢˜å’Œå‡è®¾
            2. ç ”ç©¶æ–¹æ³•
            3. ä¸»è¦å‘ç°å’Œç»“è®º
            4. ç›¸å…³ç ”ç©¶å’Œæ–‡çŒ®å¼•ç”¨

            ä»¥å­¦æœ¯æ ¼å¼è¿”å›ç»“æœã€‚
            """

            # è°ƒç”¨DeepSeek APIè¿›è¡Œå­¦æœ¯æ–‡æ¡£åˆ†æ
            try:
                ai_service = get_ai_service("deepseek")
                analysis_result = ai_service.analyze_document(chunk, "academic")

                if analysis_result["success"]:
                    print(f"âœ… [å­¦æœ¯åˆ†ææˆåŠŸ] ç¬¬{i+1}ä¸ªåˆ†å—åˆ†æå®Œæˆ")
                    # æ‰“å°AIè¿”å›çš„è¯¦ç»†ç»“æœ
                    self._print_academic_analysis_result(analysis_result, i + 1)

                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": analysis_result["content"],
                        "raw_response": analysis_result,
                        "usage": analysis_result.get("usage", {})
                    })
                else:
                    processed_chunks.append({
                        "chunk_id": i,
                        "content_preview": chunk[:100] + "...",
                        "analysis": "å­¦æœ¯æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†",
                        "error": analysis_result.get("error", "æœªçŸ¥é”™è¯¯")
                    })
            except Exception as e:
                processed_chunks.append({
                    "chunk_id": i,
                    "content_preview": chunk[:100] + "...",
                    "analysis": "å­¦æœ¯æ–‡æ¡£åˆ†æç»“æœï¼ˆé™çº§æ¨¡å¼ï¼‰",
                    "fallback": True,
                    "error": str(e)
                })

        return {
            "strategy": "academic",
            "total_chunks": len(chunks),
            "processed_chunks": processed_chunks,
            "research_contributions": [f"å­¦æœ¯è´¡çŒ® {i+1}" for i in range(min(3, len(chunks)))]
        }

    def get_chunk_size(self, document: Document) -> int:
        """å­¦æœ¯æ–‡æ¡£éœ€è¦ä¿æŒè®ºè¯å®Œæ•´æ€§ï¼Œä½¿ç”¨è¾ƒå¤§åˆ†å—"""
        return 4000

    def _split_document(self, content: str, chunk_size: int) -> List[str]:
        """æŒ‰ç« èŠ‚åˆ†å‰²å­¦æœ¯æ–‡æ¡£"""
        import re

        # åŒ¹é…ç« èŠ‚æ ‡é¢˜ (å¦‚ 1. Introduction, 2. Related Work, etc.)
        section_pattern = r'\n\d+\.\s+[^\n]+'
        sections = re.split(section_pattern, content)

        if len(sections) <= 1:
            # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ç« èŠ‚ç»“æ„ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            return self._split_by_paragraphs(content, chunk_size)

        chunks = []
        current_chunk = sections[0]

        for i, section in enumerate(sections[1:], 1):
            # é‡æ–°æ·»åŠ ç« èŠ‚æ ‡é¢˜
            section_title = re.search(section_pattern, content[i*100:])  # ç®€åŒ–å¤„ç†
            if section_title:
                section = section_title.group(0) + section

            if len(current_chunk) + len(section) <= chunk_size:
                current_chunk += section
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = section

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks or [content]

    def _split_by_paragraphs(self, content: str, chunk_size: int) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²æ–‡æ¡£"""
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""

        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) <= chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _print_academic_analysis_result(self, analysis_result: Dict[str, Any], chunk_num: int):
        """
        ğŸ“ æ‰“å°å­¦æœ¯æ–‡æ¡£AIåˆ†æçš„JSONç»“æ„åŒ–æ•°æ®

        ä¸“é—¨å±•ç¤ºå­¦æœ¯æ–‡æ¡£åˆ†æç»“æœï¼ŒåŒ…æ‹¬ç ”ç©¶æ–¹æ³•ã€ç»“è®ºã€æ–‡çŒ®å¼•ç”¨ç­‰

        Args:
            analysis_result: AIåˆ†æçš„å®Œæ•´å“åº”ç»“æœ
            chunk_num: å½“å‰åˆ†å—ç¼–å·
        """
        print(f"\nğŸ“ [å­¦æœ¯åˆ†æç»“æœ] ç¬¬{chunk_num}ä¸ªåˆ†å—çš„è¯¦ç»†å­¦æœ¯åˆ†æ:")
        print("=" * 50)

        try:
            # 1. æ‰“å°å®Œæ•´å“åº”
            print(f"ğŸ—‚ï¸  [å®Œæ•´å“åº”] AIè¿”å›çš„å­¦æœ¯åˆ†æç»“æ„:")
            formatted_response = json.dumps(analysis_result, indent=2, ensure_ascii=False)
            print(formatted_response)

            # 2. å­¦æœ¯å†…å®¹åˆ†æ
            if "content" in analysis_result:
                print(f"\nğŸ“š [å­¦æœ¯å†…å®¹] AIæå–çš„å­¦æœ¯ä¿¡æ¯:")
                print("-" * 30)
                content = analysis_result["content"]

                if isinstance(content, str):
                    try:
                        # å°è¯•è§£æJSONæ ¼å¼
                        parsed_content = json.loads(content)
                        print(f"âœ… [JSONè§£ææˆåŠŸ] å­¦æœ¯åˆ†æä¸ºJSONæ ¼å¼:")
                        print(json.dumps(parsed_content, indent=2, ensure_ascii=False))

                        # å­¦æœ¯ç‰¹å®šå­—æ®µæå–
                        if isinstance(parsed_content, dict):
                            print(f"\nğŸ” [å­¦æœ¯è¦ç´ æå–]:")
                            academic_keywords = ['research', 'method', 'conclusion', 'citation', 'study', 'hypothesis', 'finding']
                            for key, value in parsed_content.items():
                                if any(keyword in key.lower() for keyword in academic_keywords):
                                    print(f"  ğŸ“ {key}: {value}")
                                else:
                                    print(f"  ğŸ“Œ {key}: {value}")
                    except json.JSONDecodeError:
                        print(f"ğŸ“„ [å­¦æœ¯æ–‡æ¡£] åˆ†æç»“æœ:")
                        print(content)

                        # å­¦æœ¯æŒ‡æ ‡æœç´¢
                        academic_indicators = ['ç ”ç©¶', 'æ–¹æ³•', 'ç»“è®º', 'å®éªŒ', 'æ•°æ®', 'æ–‡çŒ®', 'å‡è®¾', 'å‘ç°', 'theory', 'research', 'method']
                        found_indicators = [indicator for indicator in academic_indicators if indicator in content]
                        if found_indicators:
                            print(f"ğŸ” [è¯†åˆ«çš„å­¦æœ¯æŒ‡æ ‡] {', '.join(found_indicators[:5])}")
                else:
                    print(f"ğŸ“Š [æ•°æ®å†…å®¹] {content}")

            # 3. å­¦æœ¯å…ƒæ•°æ®
            print(f"\nğŸ“Š [å­¦æœ¯å…ƒæ•°æ®]:")

            if "usage" in analysis_result:
                usage = analysis_result["usage"]
                print(f"  ğŸ’° Tokenä½¿ç”¨:")
                print(f"    è¾“å…¥: {usage.get('prompt_tokens', 0)} | è¾“å‡º: {usage.get('completion_tokens', 0)} | æ€»è®¡: {usage.get('total_tokens', 0)}")

            # 4. å­¦æœ¯åˆ†æè´¨é‡è¯„ä¼°
            content_length = len(str(analysis_result.get("content", "")))
            print(f"  ğŸ“ å­¦æœ¯æ–‡æ¡£é•¿åº¦: {content_length} å­—ç¬¦")

            # æ£€æµ‹å­¦æœ¯æ€§æŒ‡æ ‡
            content = str(analysis_result.get("content", ""))
            academic_markers = ['å¼•ç”¨', 'ç ”ç©¶', 'å®éªŒ', 'æ•°æ®', 'åˆ†æ', 'ç»“è®º', 'citation', 'reference', 'study', 'analysis']
            academic_score = sum(1 for marker in academic_markers if marker in content.lower())
            print(f"  ğŸ¯ å­¦æœ¯æ€§è¯„åˆ†: {academic_score}/10 (åŸºäºå…³é”®è¯å‡ºç°)")

            if academic_score >= 5:
                print(f"  âœ… å­¦æœ¯æ€§: å¼º")
            elif academic_score >= 3:
                print(f"  âš ï¸  å­¦æœ¯æ€§: ä¸­ç­‰")
            else:
                print(f"  âŒ å­¦æœ¯æ€§: è¾ƒå¼±")

        except Exception as e:
            print(f"âŒ [å­¦æœ¯åˆ†æé”™è¯¯] æ— æ³•è§£æå­¦æœ¯åˆ†æç»“æœ: {str(e)}")

        print("=" * 50)
        print(f"ğŸ [å­¦æœ¯åˆ†æå®Œæˆ] ç¬¬{chunk_num}ä¸ªå­¦æœ¯åˆ†æå±•ç¤ºå®Œæ¯•\n")


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ - ç­–ç•¥æ¨¡å¼ä¸­çš„"ä¸Šä¸‹æ–‡"ç±»

    è¿™æ˜¯ç­–ç•¥æ¨¡å¼çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
    1. ç®¡ç†æ‰€æœ‰å¯ç”¨çš„ç­–ç•¥
    2. æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆé€‚çš„ç­–ç•¥
    3. åè°ƒç­–ç•¥çš„æ‰§è¡Œ
    4. æä¾›ç»Ÿä¸€çš„å¤„ç†æ¥å£

    è®¾è®¡æ¨¡å¼è§’è‰²ï¼š
    - Context (ä¸Šä¸‹æ–‡): æŒæœ‰ç­–ç•¥å¯¹è±¡çš„å¼•ç”¨
    - ç­–ç•¥çš„æ‰§è¡Œè€…å’Œç®¡ç†è€…
    - å®¢æˆ·ä»£ç ä¸å…·ä½“ç­–ç•¥ä¹‹é—´çš„æ¡¥æ¢
    """

    def __init__(self):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨

        åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£å¤„ç†ç­–ç•¥å®ä¾‹
        è¿™äº›ç­–ç•¥å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ¨æ€é€‰æ‹©å’Œåˆ‡æ¢
        """
        print("ğŸ­ [å¤„ç†å™¨åˆå§‹åŒ–] åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨å®ä¾‹")
        print("ğŸ“‹ [ç­–ç•¥æ³¨å†Œ] æ³¨å†Œæ‰€æœ‰å¯ç”¨çš„å¤„ç†ç­–ç•¥...")

        # ğŸ”§ ç­–ç•¥æ³¨å†Œè¡¨ - ç­–ç•¥æ¨¡å¼çš„æ ¸å¿ƒæ•°æ®ç»“æ„
        self.strategies = {
            "legal": LegalDocumentStrategy(),        # æ³•å¾‹æ–‡æ¡£ç­–ç•¥
            "technical": TechnicalDocumentStrategy(), # æŠ€æœ¯æ–‡æ¡£ç­–ç•¥
            "academic": AcademicDocumentStrategy(),   # å­¦æœ¯æ–‡æ¡£ç­–ç•¥
            "general": TechnicalDocumentStrategy()    # é€šç”¨ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰
        }

        print(f"âœ… [ç­–ç•¥æ³¨å†Œå®Œæˆ] å…±æ³¨å†Œ {len(self.strategies)} ä¸ªç­–ç•¥:")
        for name, strategy in self.strategies.items():
            print(f"  ğŸ“ {name}: {strategy.__class__.__name__}")

    def process_document(self, document: Document, strategy_name: str = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£çš„ä¸»è¦æ–¹æ³• - ç­–ç•¥æ¨¡å¼çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘

        å¤„ç†æµç¨‹ï¼š
        1. ç­–ç•¥é€‰æ‹© (è‡ªåŠ¨æˆ–æ‰‹åŠ¨)
        2. ç­–ç•¥éªŒè¯ (ç¡®ä¿ç­–ç•¥å­˜åœ¨)
        3. ç­–ç•¥æ‰§è¡Œ (è°ƒç”¨å…·ä½“ç­–ç•¥çš„processæ–¹æ³•)
        4. ç»“æœè¿”å›

        Args:
            document: å¾…å¤„ç†çš„æ–‡æ¡£å¯¹è±¡
            strategy_name: å¯é€‰çš„ç­–ç•¥åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        print(f"\nğŸ¯ [å¼€å§‹å¤„ç†] ====================")
        print(f"ğŸ“„ [æ–‡æ¡£ä¿¡æ¯] æ ‡é¢˜: {document.title}")
        print(f"ğŸ“Š [æ–‡æ¡£ç»Ÿè®¡] é•¿åº¦: {document.length} å­—ç¬¦")

        # ğŸ” ç­–ç•¥é€‰æ‹©é€»è¾‘
        if strategy_name is None:
            # è‡ªåŠ¨ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹
            strategy_name = document.doc_type
            print(f"ğŸ¤– [ç­–ç•¥é€‰æ‹©] è‡ªåŠ¨é€‰æ‹©ç­–ç•¥: {strategy_name} (åŸºäºæ–‡æ¡£ç±»å‹)")
        else:
            # æ‰‹åŠ¨ç­–ç•¥æŒ‡å®š
            print(f"ğŸ‘¤ [ç­–ç•¥æŒ‡å®š] æ‰‹åŠ¨æŒ‡å®šç­–ç•¥: {strategy_name}")

        # âœ… ç­–ç•¥éªŒè¯
        if strategy_name not in self.strategies:
            print(f"âš ï¸  [ç­–ç•¥è­¦å‘Š] æœªçŸ¥ç­–ç•¥ '{strategy_name}'ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            strategy_name = "general"

        # ğŸ¯ è·å–ç­–ç•¥å®ä¾‹
        strategy = self.strategies[strategy_name]
        print(f"ğŸ”§ [ç­–ç•¥å®ä¾‹] ä½¿ç”¨ç­–ç•¥: {strategy.__class__.__name__}")

        try:
            # ğŸš€ æ‰§è¡Œç­–ç•¥ - è¿™æ˜¯ç­–ç•¥æ¨¡å¼çš„å…³é”®è°ƒç”¨ç‚¹
            print(f"âš¡ [ç­–ç•¥æ‰§è¡Œ] å¼€å§‹æ‰§è¡Œ {strategy_name} ç­–ç•¥...")
            start_time = time.time()

            result = strategy.process(document)

            end_time = time.time()
            processing_time = end_time - start_time

            # ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡
            print(f"â±ï¸  [æ‰§è¡Œæ—¶é—´] å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’")
            print(f"ğŸ“ˆ [å¤„ç†ç»Ÿè®¡] æ€»åˆ†å—æ•°: {result.get('total_chunks', 0)}")

            if 'success_rate' in result:
                print(f"ğŸ¯ [æˆåŠŸç‡] {result['success_rate']:.1%}")

            print(f"âœ… [å¤„ç†å®Œæˆ] æ–‡æ¡£å¤„ç†æˆåŠŸ!")
            print(f"ğŸ† [ä½¿ç”¨ç­–ç•¥] {result['strategy']} ç­–ç•¥")

            return result

        except Exception as e:
            print(f"ğŸ’¥ [å¤„ç†å¼‚å¸¸] ç­–ç•¥æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def add_strategy(self, name: str, strategy: DocumentProcessingStrategy):
        """
        åŠ¨æ€æ·»åŠ æ–°çš„å¤„ç†ç­–ç•¥

        è¿™å±•ç¤ºäº†ç­–ç•¥æ¨¡å¼çš„æ‰©å±•æ€§ï¼š
        - å¯ä»¥åœ¨è¿è¡Œæ—¶æ·»åŠ æ–°ç­–ç•¥
        - ä¸éœ€è¦ä¿®æ”¹ç°æœ‰ä»£ç 
        - ç¬¦åˆå¼€é—­åŸåˆ™

        Args:
            name: ç­–ç•¥åç§°
            strategy: ç­–ç•¥å®ä¾‹
        """
        print(f"â• [ç­–ç•¥æ‰©å±•] æ·»åŠ æ–°ç­–ç•¥: {name} -> {strategy.__class__.__name__}")
        self.strategies[name] = strategy
        print(f"âœ… [æ‰©å±•å®Œæˆ] ç­–ç•¥æ± ç°åœ¨åŒ…å« {len(self.strategies)} ä¸ªç­–ç•¥")

    def list_available_strategies(self):
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç­–ç•¥

        ç”¨äºè°ƒè¯•å’Œäº†è§£å½“å‰æ”¯æŒçš„æ–‡æ¡£ç±»å‹
        """
        print(f"\nğŸ“‹ [å¯ç”¨ç­–ç•¥] å½“å‰æ”¯æŒçš„æ–‡æ¡£å¤„ç†ç­–ç•¥:")
        for name, strategy in self.strategies.items():
            print(f"  ğŸ·ï¸  {name}: {strategy.__class__.__name__}")

    def get_strategy_info(self, strategy_name: str) -> Dict[str, Any]:
        """
        è·å–ç‰¹å®šç­–ç•¥çš„ä¿¡æ¯

        Args:
            strategy_name: ç­–ç•¥åç§°

        Returns:
            Dict[str, Any]: ç­–ç•¥ä¿¡æ¯
        """
        if strategy_name not in self.strategies:
            return {"error": f"ç­–ç•¥ '{strategy_name}' ä¸å­˜åœ¨"}

        strategy = self.strategies[strategy_name]
        return {
            "name": strategy_name,
            "class": strategy.__class__.__name__,
            "module": strategy.__class__.__module__,
            "doc": strategy.__class__.__doc__ or "æ— æ–‡æ¡£"
        }


# ============================================================================
# ç­–ç•¥æ¨¡å¼æ¼”ç¤ºå’Œæµ‹è¯•
# ============================================================================

def demo_strategy_pattern():
    """
    ç­–ç•¥æ¨¡å¼å®Œæ•´æ¼”ç¤º

    è¿™ä¸ªæ¼”ç¤ºå‡½æ•°å±•ç¤ºäº†ç­–ç•¥æ¨¡å¼çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š
    1. åˆ›å»ºå¤„ç†å™¨ï¼ˆä¸Šä¸‹æ–‡ï¼‰
    2. åˆ›å»ºä¸åŒç±»å‹çš„æ–‡æ¡£
    3. ä½¿ç”¨ä¸åŒç­–ç•¥å¤„ç†æ–‡æ¡£
    4. å±•ç¤ºç­–ç•¥é€‰æ‹©çš„çµæ´»æ€§
    """
    print("ğŸ¯ [æ¼”ç¤ºå¼€å§‹] ç­–ç•¥æ¨¡å¼å®Œæ•´æ¼”ç¤º")
    print("=" * 60)

    # æ­¥éª¤1: åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨ï¼ˆç­–ç•¥æ¨¡å¼çš„Contextï¼‰
    print("\nğŸ“‹ [æ­¥éª¤1] åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨")
    processor = DocumentProcessor()

    # å±•ç¤ºæ‰€æœ‰å¯ç”¨ç­–ç•¥
    processor.list_available_strategies()

    # æ­¥éª¤2: åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•æ–‡æ¡£
    print("\nğŸ“„ [æ­¥éª¤2] åˆ›å»ºæµ‹è¯•æ–‡æ¡£")

    # æ³•å¾‹æ–‡æ¡£
    legal_doc = Document(
        content="""è½¯ä»¶è®¸å¯åè®®

ç¬¬ä¸€æ¡ è®¸å¯æˆäºˆ
æœ¬å…¬å¸ç‰¹æ­¤æˆäºˆæ‚¨ä½¿ç”¨æœ¬è½¯ä»¶çš„éç‹¬å æ€§è®¸å¯ã€‚

ç¬¬äºŒæ¡ ä½¿ç”¨é™åˆ¶
1. è¢«è®¸å¯æ–¹ä¸å¾—å¯¹è½¯ä»¶è¿›è¡Œåå‘å·¥ç¨‹
2. è¢«è®¸å¯æ–¹ä¸å¾—å°†è½¯ä»¶ç”¨äºå•†ä¸šç›®çš„

ç¬¬ä¸‰æ¡ çŸ¥è¯†äº§æƒ
è½¯ä»¶çš„æ‰€æœ‰çŸ¥è¯†äº§æƒå½’è®¸å¯æ–¹æ‰€æœ‰""",
        title="è½¯ä»¶è®¸å¯åè®®",
        doc_type="legal"
    )

    # æŠ€æœ¯æ–‡æ¡£
    tech_doc = Document(
        content="""# APIæ¥å£æ–‡æ¡£

## ç”¨æˆ·è®¤è¯

ä½¿ç”¨JWT tokenè¿›è¡Œèº«ä»½éªŒè¯ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
Authorization: Bearer <token>
```

## æ¥å£åˆ—è¡¨

### GET /api/users
è·å–ç”¨æˆ·åˆ—è¡¨

**å‚æ•°ï¼š**
- page: é¡µç  (é»˜è®¤1)
- size: æ¯é¡µæ•°é‡ (é»˜è®¤10)

**è¿”å›ï¼š** ç”¨æˆ·åˆ—è¡¨JSON""",
        title="ç”¨æˆ·æœåŠ¡APIæ–‡æ¡£",
        doc_type="technical"
    )

    # å­¦æœ¯æ–‡æ¡£
    academic_doc = Document(
        content="""1. å¼•è¨€

æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨ã€‚

2. ç›¸å…³å·¥ä½œ

Smith et al. (2020) æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

3. æ–¹æ³•

æˆ‘ä»¬é‡‡ç”¨Transformeræ¶æ„ï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ”¹è¿›ã€‚

4. ç»“è®º

å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡""",
        title="åŸºäºæ·±åº¦å­¦ä¹ çš„NLPç ”ç©¶",
        doc_type="academic"
    )

    # é€šç”¨æ–‡æ¡£ï¼ˆç”¨äºæµ‹è¯•é»˜è®¤ç­–ç•¥ï¼‰
    general_doc = Document(
        content="è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„æ–‡æ¡£å†…å®¹ï¼Œç”¨äºæµ‹è¯•é»˜è®¤çš„å¤„ç†ç­–ç•¥ã€‚",
        title="æ™®é€šæ–‡æ¡£",
        doc_type="general"
    )

    # æ˜¾ç¤ºæ–‡æ¡£ä¿¡æ¯
    documents = [legal_doc, tech_doc, academic_doc, general_doc]
    for i, doc in enumerate(documents, 1):
        print(f"  ğŸ“„ æ–‡æ¡£{i}: {doc.title} (ç±»å‹: {doc.doc_type}, é•¿åº¦: {doc.length} å­—ç¬¦)")

    # æ­¥éª¤3: ä½¿ç”¨ä¸åŒç­–ç•¥å¤„ç†æ–‡æ¡£
    print("\nğŸ”§ [æ­¥éª¤3] æ‰§è¡Œç­–ç•¥å¤„ç†")

    for i, doc in enumerate(documents, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ [æ–‡æ¡£{i}] å¤„ç†å¼€å§‹: {doc.title}")
        print(f"{'='*60}")

        # æ¼”ç¤ºè‡ªåŠ¨ç­–ç•¥é€‰æ‹©
        result = processor.process_document(doc)

        # æ¼”ç¤ºæ‰‹åŠ¨ç­–ç•¥æŒ‡å®š
        if i == 1:  # å¯¹ç¬¬ä¸€ä¸ªæ–‡æ¡£ä¹Ÿå°è¯•æ‰‹åŠ¨æŒ‡å®šç­–ç•¥
            print(f"\nğŸ”„ [ç­–ç•¥åˆ‡æ¢] æ‰‹åŠ¨æŒ‡å®šæŠ€æœ¯ç­–ç•¥å¤„ç†æ³•å¾‹æ–‡æ¡£...")
            manual_result = processor.process_document(doc, "technical")
            print(f"ğŸ“Š [å¯¹æ¯”ç»“æœ] åŸç­–ç•¥: {result['strategy']}, æ‰‹åŠ¨ç­–ç•¥: {manual_result['strategy']}")

    # æ­¥éª¤4: æ¼”ç¤ºç­–ç•¥æ‰©å±•
    print(f"\n{'='*60}")
    print("ğŸ”§ [æ­¥éª¤4] æ¼”ç¤ºç­–ç•¥æ‰©å±•æ€§")
    print(f"{'='*60}")

    # åˆ›å»ºè‡ªå®šä¹‰ç­–ç•¥
    class CustomMarketingStrategy(DocumentProcessingStrategy):
        """è¥é”€æ–‡æ¡£ç­–ç•¥ - è‡ªå®šä¹‰ç­–ç•¥æ¼”ç¤º"""

        def process(self, document: Document) -> Dict[str, Any]:
            print(f"ğŸ“ˆ [è‡ªå®šä¹‰ç­–ç•¥] ä½¿ç”¨è¥é”€ç­–ç•¥å¤„ç†: {document.title}")
            return {
                "strategy": "marketing",
                "total_chunks": 1,
                "processed_chunks": [{
                    "chunk_id": 0,
                    "analysis": "è¥é”€æ–‡æ¡£åˆ†æï¼šè¯†åˆ«æ¨å¹¿é‡ç‚¹ã€ç›®æ ‡å—ä¼—ã€è¥é”€æ¸ é“ç­‰"
                }]
            }

        def get_chunk_size(self, document: Document) -> int:
            return 1500

    # æ·»åŠ è‡ªå®šä¹‰ç­–ç•¥
    processor.add_strategy("marketing", CustomMarketingStrategy())

    # ä½¿ç”¨æ–°ç­–ç•¥
    marketing_doc = Document(
        content="è¿™æ˜¯ä¸€ä»½è¥é”€ç­–åˆ’æ–¹æ¡ˆï¼ŒåŒ…å«äº§å“æ¨å¹¿ç­–ç•¥å’Œå¸‚åœºåˆ†æã€‚",
        title="è¥é”€ç­–åˆ’æ¡ˆ",
        doc_type="marketing"
    )

    print(f"ğŸ“„ [ä½¿ç”¨æ–°ç­–ç•¥] å¤„ç†è¥é”€æ–‡æ¡£...")
    marketing_result = processor.process_document(marketing_doc)
    print(f"ğŸ“Š [æ–°ç­–ç•¥ç»“æœ] è¥é”€ç­–ç•¥å¤„ç†å®Œæˆ")

    # æ­¥éª¤5: ç­–ç•¥ä¿¡æ¯æŸ¥çœ‹
    print(f"\n{'='*60}")
    print("ğŸ“‹ [æ­¥éª¤5] ç­–ç•¥ä¿¡æ¯æŸ¥çœ‹")
    print(f"{'='*60}")

    for strategy_name in ["legal", "technical", "academic", "marketing"]:
        info = processor.get_strategy_info(strategy_name)
        print(f"ğŸ·ï¸  {strategy_name}: {info['class']}")

    # æ¼”ç¤ºæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ‰ [æ¼”ç¤ºå®Œæˆ] ç­–ç•¥æ¨¡å¼æ¼”ç¤ºæ€»ç»“")
    print(f"{'='*60}")
    print("""
ğŸ’¡ ç­–ç•¥æ¨¡å¼æ ¸å¿ƒè¦ç‚¹ï¼š

1. ğŸ¯ ç­–ç•¥æ¥å£ (DocumentProcessingStrategy)
   - å®šä¹‰ç»Ÿä¸€çš„å¤„ç†æ¥å£
   - æ‰€æœ‰å…·ä½“ç­–ç•¥éƒ½å®ç°ç›¸åŒæ¥å£

2. ğŸ­ ä¸Šä¸‹æ–‡ç±» (DocumentProcessor)
   - ç®¡ç†ç­–ç•¥é›†åˆ
   - è´Ÿè´£ç­–ç•¥é€‰æ‹©å’Œæ‰§è¡Œ
   - æä¾›ç»Ÿä¸€çš„å®¢æˆ·ç«¯æ¥å£

3. ğŸ”§ å…·ä½“ç­–ç•¥ (LegalDocumentStrategy, TechnicalDocumentStrategy, ...)
   - å®ç°å…·ä½“çš„å¤„ç†é€»è¾‘
   - æ¯ç§ç­–ç•¥ä¸“æ³¨äºç‰¹å®šç±»å‹æ–‡æ¡£
   - å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•

4. ğŸš€ ç­–ç•¥é€‰æ‹©
   - è¿è¡Œæ—¶åŠ¨æ€é€‰æ‹©ç­–ç•¥
   - åŸºäºæ–‡æ¡£ç±»å‹è‡ªåŠ¨é€‰æ‹©
   - æ”¯æŒæ‰‹åŠ¨æŒ‡å®šç­–ç•¥

5. ğŸ”Œ æ‰©å±•æ€§
   - æ·»åŠ æ–°ç­–ç•¥æ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç 
   - ç¬¦åˆå¼€é—­åŸåˆ™
   - æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€æ³¨å†Œ

ç­–ç•¥æ¨¡å¼çš„ä¼˜åŠ¿ï¼š
âœ… ç®—æ³•æ—å°è£…
âœ… è¿è¡Œæ—¶ç­–ç•¥åˆ‡æ¢
âœ… æ˜“äºæ‰©å±•å’Œç»´æŠ¤
âœ… é¿å…æ¡ä»¶åˆ†æ”¯
âœ… ç¬¦åˆSOLIDåŸåˆ™
""")


if __name__ == "__main__":
    # è¿è¡Œç­–ç•¥æ¨¡å¼æ¼”ç¤º
    demo_strategy_pattern()